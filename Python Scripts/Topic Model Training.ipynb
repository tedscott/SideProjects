{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "31b17f54-6bfd-48da-ade1-d08773cc47f1"
    }
   },
   "source": [
    "## Automatic Learning of Key Phrases and Topics in Document Collections\n",
    "\n",
    "## Part 3: Topic Modeling Training and Summarization\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is Part 3 of 4, in a series providing a step-by-step description of how to process and analyze the contents of a large collection of text documents in an unsupervised manner. Using Python packages and custom code examples, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology.\n",
    "\n",
    "Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range natural language data including news stories, legal documents, research publications, social media forum discussion, customer feedback forms, product reviews, and many more.\n",
    "\n",
    "Part 3 of the series shows how to train a topic model on a collection of text documents and how to use the topic model to summarize the contents of the corpus. The training is applied to text generated from the preprocessing and phrase learning stages presented in Parts 1 and 2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "21d8d7fd-501d-4eec-abd4-a827100e5406"
    }
   },
   "source": [
    "### Import Relevant Python Packages\n",
    "\n",
    "Most significantly, Part 3 relies on the use of the [Gensim Python library](http://radimrehurek.com/gensim/)  for generating a sparse bag-of-words representation of each document and then training a [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) model on the data. LDA produces a collection of latent topics learned in a completely unsupervised fashion from the text data. Each document can then be represented with a distribution of the learned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "23d9b7ef-11db-419b-9f97-7bb07a1e3ca5"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas \n",
    "import re\n",
    "import math\n",
    "from gensim import corpora, models\n",
    "from operator import itemgetter\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "44f6c13e-aae6-4bcf-97f5-5527bacc0512"
    }
   },
   "source": [
    "### Load Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "46697539-829d-406f-8722-7b4f7ec3beee"
    }
   },
   "outputs": [],
   "source": [
    "# Load full TSV file including a column of text\n",
    "frame = pandas.read_csv(\"../Data/CongressionalDocsProcessed.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fb58ed5d-85bb-4ab3-9b8e-2d8b79d2df55"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs in corpus: 189088\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>ProcessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hconres1-100</td>\n",
       "      <td>provides_for_a_joint session_of_the_congress o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hconres1-101</td>\n",
       "      <td>salvadoran foreign_assistance reform resolutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hconres1-102</td>\n",
       "      <td>supports the president's actions to defend sau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hconres1-103</td>\n",
       "      <td>declares that it is the sense_of_the_congress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hconres1-104</td>\n",
       "      <td>recognizes the sacrifice of army chief warrant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          DocID                                      ProcessedText\n",
       "0  hconres1-100  provides_for_a_joint session_of_the_congress o...\n",
       "1  hconres1-101  salvadoran foreign_assistance reform resolutio...\n",
       "2  hconres1-102  supports the president's actions to defend sau...\n",
       "3  hconres1-103  declares that it is the sense_of_the_congress ...\n",
       "4  hconres1-104  recognizes the sacrifice of army chief warrant..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Total docs in corpus: %d\\n\" % len(frame))\n",
    "\n",
    "# Show the first five rows of the data in the frame\n",
    "frame[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bc5ecdb1-c51c-457f-be3f-eee94247f13b"
    }
   },
   "source": [
    "### Load the Stop Word Lists\n",
    "Latent topic models attempt to restrict the topic learning processing to the use of only content bearing words by excluding non-content bearing <b><i>stop words</i></b>. Manually crafted stop word lists are typically manually crafted and include common functional words such as articles, conjunctions, prepositions, pronouns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "8a9b81d1-058f-4a37-acbe-b825045866b4"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function for loading lists into dictionary hash tables\n",
    "def LoadListAsHash(filename):\n",
    "    listHash = {}\n",
    "    fp = open(filename)\n",
    "\n",
    "    # Read in lines one by one stripping away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in fp.readlines():\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "\n",
    "    fp.close()\n",
    "    return listHash "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bc5e3810-08b3-41aa-af97-0159bf9be597"
    }
   },
   "outputs": [],
   "source": [
    "# Load the stop-list of non-content bearing function words\n",
    "stopwordHash = LoadListAsHash(\"../Data/function_words.txt\")\n",
    "\n",
    "# Additional words can also be manually added to the stop word list as needed\n",
    "stopwordHash[\"foo\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3593bde-7eee-4a9d-8565-7d9774947404"
    }
   },
   "source": [
    "### Load the Mapping of Lower-Cased Vocabulary Items to Their Most Common Surface Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "21678e5f-ad6c-46d1-8a71-f64bd9483c42"
    }
   },
   "outputs": [],
   "source": [
    "# Load surface form mappings here\n",
    "fp = open(\"../Data/Vocab2SurfaceFormMapping.tsv\")\n",
    "\n",
    "vocabToSurfaceFormHash = {}\n",
    "\n",
    "# Each line in the file has two tab separated fields;\n",
    "# the first is the vocabulary item used during modeling\n",
    "# and the second is its most common surface form in the \n",
    "# original data\n",
    "for stringIn in fp.readlines():\n",
    "    fields = stringIn.strip().split(\"\\t\")\n",
    "    if len(fields) != 2:\n",
    "        print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "    elif fields[0] == \"\" or fields[1] == \"\":\n",
    "        print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
    "    else:\n",
    "        vocabToSurfaceFormHash[fields[0]] = fields[1]\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ddff9a8-0a31-4889-bedb-b64d10eddcc1"
    }
   },
   "source": [
    "### Do Topic Modeling on Corpus using Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "177ee67d-3e38-4009-b65e-4d08f79c862b"
    }
   },
   "source": [
    "#### Create the Vocabulary Used for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "55d8b0a5-11a9-4cf1-8399-b2d036254dd0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words\n",
      "Building vocab\n",
      "Excluded 293 stop words\n",
      "Excluded 9337 non-alphabetic words\n",
      "Excluded 64323 words below word count threshold\n",
      "Excluded 285 words below doc count threshold\n",
      "Excluded 1 words above max doc frequency\n",
      "Final Vocab Size: 73672 words\n"
     ]
    }
   ],
   "source": [
    "def CreateVocabForTopicModeling(textData,stopwordHash):\n",
    "\n",
    "    print (\"Counting words\")\n",
    "    numDocs = len(textData) \n",
    "    globalWordCountHash = {} \n",
    "    globalDocCountHash = {} \n",
    "    for textLine in textData:\n",
    "        docWordCountHash = {}\n",
    "        for word in textLine.split():\n",
    "            if word in globalWordCountHash:\n",
    "                globalWordCountHash[word] += 1\n",
    "            else:\n",
    "                globalWordCountHash[word] = 1\n",
    "            if word not in docWordCountHash: \n",
    "                docWordCountHash[word] = 1\n",
    "                if word in globalDocCountHash:\n",
    "                    globalDocCountHash[word] += 1\n",
    "                else:\n",
    "                    globalDocCountHash[word] = 1\n",
    "\n",
    "    minWordCount = 5;\n",
    "    minDocCount = 2;\n",
    "    maxDocFreq = .25;\n",
    "    vocabCount = 0;\n",
    "    vocabHash = {}\n",
    "\n",
    "    excStopword = 0\n",
    "    excNonalphabetic = 0\n",
    "    excMinwordcount = 0\n",
    "    excNotindochash = 0\n",
    "    excMindoccount = 0\n",
    "    excMaxdocfreq =0\n",
    "\n",
    "    print (\"Building vocab\")\n",
    "    for word in globalWordCountHash.keys():\n",
    "        # Test vocabulary exclusion criteria for each word\n",
    "        if ( word in stopwordHash ):\n",
    "            excStopword += 1\n",
    "        elif ( not re.search(r'[a-zA-Z]', word, 0) ):\n",
    "            excNonalphabetic += 1\n",
    "        elif ( globalWordCountHash[word] < minWordCount ):\n",
    "            excMinwordcount += 1\n",
    "        elif ( word not in globalDocCountHash ):\n",
    "            print (\"Warning: Word '%s' not in doc count hash\") % (word)\n",
    "            excNotindochash += 1\n",
    "        elif ( globalDocCountHash[word] < minDocCount ):\n",
    "            excMindoccount += 1\n",
    "        elif ( float(globalDocCountHash[word])/float(numDocs) > maxDocFreq ):\n",
    "            excMaxdocfreq += 1\n",
    "        else:\n",
    "            # Add word to vocab\n",
    "            vocabHash[word]= globalWordCountHash[word];\n",
    "            vocabCount += 1 \n",
    "    print (\"Excluded %d stop words\" % (excStopword))       \n",
    "    print (\"Excluded %d non-alphabetic words\" % (excNonalphabetic))  \n",
    "    print (\"Excluded %d words below word count threshold\" % (excMinwordcount)) \n",
    "    print (\"Excluded %d words below doc count threshold\" % (excMindoccount))\n",
    "    print (\"Excluded %d words above max doc frequency\" % (excMaxdocfreq)) \n",
    "    print (\"Final Vocab Size: %d words\" % vocabCount)\n",
    "            \n",
    "    return vocabHash\n",
    "                    \n",
    "vocabHash = CreateVocabForTopicModeling(frame['ProcessedText'],stopwordHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8e615a03-a616-4ce6-aaed-149bb7dc6c41"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show that the stop word \"and\" is not the vocabulary\n",
    "'and' in vocabHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1301be86-24d2-45a3-aced-d0b4948e765b"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a learned phrase is in the vocabulary\n",
    "'department_of_labor' in vocabHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e537aed2-414b-4e59-be59-5cee147398b5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary hash table contains the total count of the vocabulary item in the data set\n",
    "vocabHash[\"department_of_labor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "22d6a20f-fa82-4df0-8aac-29c150397a1f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('requires', 91172),\n",
       " ('program', 69610),\n",
       " ('state', 66911),\n",
       " ('including', 56521),\n",
       " ('provides', 55203),\n",
       " ('certain', 53096),\n",
       " ('provide', 48731),\n",
       " ('programs', 41210),\n",
       " ('united_states', 40047),\n",
       " ('prohibits', 39121)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the 10 most frequent non-excluded words in the vocabulary\n",
    "sorted(vocabHash.items(), key=lambda x: -x[1])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1940dcd6-9b8b-4dea-af39-ca75e72062aa"
    }
   },
   "source": [
    "#### Convert the Text Data Into a Sparse Vector Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6244226e-a91b-4767-83da-d41e882399a4"
    }
   },
   "outputs": [],
   "source": [
    "# Start by tokenizing the full text string string for each document into list of tokens\n",
    "# Any token that is in not in the pre-defined set of acceptable vocabulary words is execluded\n",
    "def TokenizeText(textData,vocabHash):\n",
    "    tokenizedText = []\n",
    "        \n",
    "    for textLine in textData:\n",
    "        tokenizedText.append([token for token in textLine.split() if token in vocabHash])    \n",
    "    return tokenizedText\n",
    "    \n",
    "tokenizedDocs = TokenizeText(frame['ProcessedText'], vocabHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f28084b0-cc2e-4231-842f-5765d92d44f0"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['provides_for_a_joint',\n",
       "  'session_of_the_congress',\n",
       "  'january_27',\n",
       "  'message_from_the_president',\n",
       "  'state_of_the_union'],\n",
       " ['salvadoran',\n",
       "  'foreign_assistance',\n",
       "  'reform',\n",
       "  'resolution',\n",
       "  'expresses_the_sense_of_the_congress',\n",
       "  'u.s._foreign_assistance_program',\n",
       "  'el_salvador',\n",
       "  'revised',\n",
       "  'promote',\n",
       "  'negotiated_settlement',\n",
       "  'reduction',\n",
       "  'human',\n",
       "  'suffering',\n",
       "  'ratio',\n",
       "  'assistance',\n",
       "  'reversed',\n",
       "  'fy',\n",
       "  'spent',\n",
       "  'war',\n",
       "  'effort',\n",
       "  'one-third',\n",
       "  'spent',\n",
       "  'reform',\n",
       "  'development_activities',\n",
       "  'assistance',\n",
       "  'distributed',\n",
       "  'manner',\n",
       "  'promote',\n",
       "  'interests',\n",
       "  'particular',\n",
       "  'political_party',\n",
       "  'assistance',\n",
       "  'distributed',\n",
       "  'church-related',\n",
       "  'nongovernmental_organizations',\n",
       "  'international_organizations',\n",
       "  'selected',\n",
       "  'agency_for_international_development',\n",
       "  'president',\n",
       "  'report_quarterly',\n",
       "  'congress',\n",
       "  'restructuring',\n",
       "  'assistance',\n",
       "  'economic',\n",
       "  'results',\n",
       "  'restructuring',\n",
       "  'reports',\n",
       "  'corruption',\n",
       "  'distribution']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the tokenizaton of the first two documents\n",
    "tokenizedDocs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c158c2fc-56b7-40a7-a765-d7a3b4692839"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of retained tokens: 15903854\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of vocabulary tokens used over the entire corpus \n",
    "numTokens = 0\n",
    "for i in range(0,len(tokenizedDocs)):\n",
    "    numTokens += len(tokenizedDocs[i])\n",
    "print(\"Total number of retained tokens: %d\" % numTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "be794056-435f-49e2-b69b-2ca5409710bf"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary mapping string tokens in the text to unique token IDs\n",
    "dictionary = corpora.Dictionary(tokenizedDocs)\n",
    "\n",
    "# If the reverse mapping for token ids back to string doesn't exist then create the mapping\n",
    "# in the form of a list where the list index is the tokenID and the list value is the token\n",
    "if len(dictionary.id2token) == 0:\n",
    "    numTokens = len(dictionary.token2id);\n",
    "    id2token = numTokens * [\"\"];\n",
    "    for token in dictionary.token2id:\n",
    "        tokenID = dictionary.token2id[token]\n",
    "        if tokenID < numTokens:\n",
    "            id2token[tokenID] = token\n",
    "        else: \n",
    "            print (\"Warning: token id %d for token '%s' exceeds max index of %d\" % (tokenID,token,numTopics-1))\n",
    "    for i in range(0,numTokens):\n",
    "        if id2token[i] == \"\":\n",
    "            print (\"Warning: token id %d has an empty token\" % i)\n",
    "else:\n",
    "    id2token = dictionary.id2token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eaf0b191-c85a-48b9-9bcb-b38ca076c9b4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID 0 --> january_27\n",
      "Token ID 1 --> state_of_the_union\n",
      "Token ID 2 --> message_from_the_president\n",
      "Token ID 3 --> session_of_the_congress\n",
      "Token ID 4 --> provides_for_a_joint\n",
      "Token ID 5 --> revised\n",
      "Token ID 6 --> one-third\n",
      "Token ID 7 --> spent\n",
      "Token ID 8 --> economic\n",
      "Token ID 9 --> manner\n",
      "\n",
      "spent --> Token ID 7\n",
      "provides_for_a_joint --> Token ID 4\n"
     ]
    }
   ],
   "source": [
    "# The mapping from unique token ids to strings uses the id2token element of the dictionary\n",
    "for i in range(0,10):\n",
    "    print (\"Token ID %d --> %s\" % (i, id2token[i]))\n",
    "\n",
    "# The mapping from strings to unique token ids uses the token2id element of the dictionary   \n",
    "print (\"\")\n",
    "print (\"%s --> Token ID %d\" % ('spent', dictionary.token2id['spent']))\n",
    "print (\"%s --> Token ID %d\" % ('provides_for_a_joint', dictionary.token2id['provides_for_a_joint']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bcd1daea-0849-4cd4-9349-a7f52641eb95"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Gensim corpus structure\n",
    "corpus =[dictionary.doc2bow(tokens) for tokens in tokenizedDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1f6a12bd-9325-457d-83d0-9e374033e64f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salvadoran', 'foreign_assistance', 'reform', 'resolution', 'expresses_the_sense_of_the_congress', 'u.s._foreign_assistance_program', 'el_salvador', 'revised', 'promote', 'negotiated_settlement', 'reduction', 'human', 'suffering', 'ratio', 'assistance', 'reversed', 'fy', 'spent', 'war', 'effort', 'one-third', 'spent', 'reform', 'development_activities', 'assistance', 'distributed', 'manner', 'promote', 'interests', 'particular', 'political_party', 'assistance', 'distributed', 'church-related', 'nongovernmental_organizations', 'international_organizations', 'selected', 'agency_for_international_development', 'president', 'report_quarterly', 'congress', 'restructuring', 'assistance', 'economic', 'results', 'restructuring', 'reports', 'corruption', 'distribution']\n",
      "spent: 7\n",
      "[(5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2), (19, 4), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Show that the corpus structure models the tokenized text as a sparse list of the tokens \n",
    "# in the document where each list item is represented by the unique ID for the token along \n",
    "# with the count of how often that token appeared in the document\n",
    "print(tokenizedDocs[1])\n",
    "print(\"spent:\", dictionary.token2id['spent'])\n",
    "print (corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0086b4c-7e4a-4bc7-b720-7ef6bf2508ad"
    }
   },
   "source": [
    "#### Train an LDA Topic Model Using the Gensim Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2152a599-0581-4406-a32c-58b4ff36711a"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4d17de12-2451-4b0b-ab3b-a73bf7f6e9dc"
    }
   },
   "outputs": [],
   "source": [
    "# Set the number of topics to be learned to 200\n",
    "numTopics=200\n",
    "\n",
    "# Train LDA model \n",
    "if False:\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=numTopics, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c96b2d58-4729-4243-8799-c21dc615fa58"
    }
   },
   "outputs": [],
   "source": [
    "# Saving and loading a trained LDA model\n",
    "ldaFile = \"../Data/CongressionalDocsLDA.pickle\"\n",
    "\n",
    "if False:\n",
    "    #Save a trained LDA model\n",
    "    lda.save(ldaFile)\n",
    "    \n",
    "else: \n",
    "    # Loaded trained model\n",
    "    lda = gensim.models.ldamodel.LdaModel.load(ldaFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d3b757c3-d523-4bc0-80af-7be110f7a46e"
    }
   },
   "source": [
    "#### Accessing the Contents of the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "96008087-cb6e-4ba1-b9c6-b4480835ef0f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__ignoreds': ['state', 'dispatcher'],\n",
       " '__numpys': ['expElogbeta'],\n",
       " '__recursive_saveloads': ['id2word'],\n",
       " '__scipys': [],\n",
       " 'alpha': array([ 0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,\n",
       "         0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005,  0.005]),\n",
       " 'chunksize': 2000,\n",
       " 'decay': 0.5,\n",
       " 'dispatcher': None,\n",
       " 'distributed': False,\n",
       " 'eta': array([[ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005],\n",
       "        [ 0.005]]),\n",
       " 'eval_every': 10,\n",
       " 'expElogbeta': array([[  1.72956242e-92,   1.72956242e-92,   1.72956242e-92, ...,\n",
       "           1.72956242e-92,   1.72956242e-92,   1.72956242e-92],\n",
       "        [  2.17668948e-92,   2.17668948e-92,   2.17668948e-92, ...,\n",
       "           2.17668948e-92,   2.17668948e-92,   2.17668948e-92],\n",
       "        [  1.22902302e-92,   1.22902302e-92,   1.22902302e-92, ...,\n",
       "           1.22902302e-92,   1.22902302e-92,   1.22902302e-92],\n",
       "        ..., \n",
       "        [  2.05134460e-92,   2.05134460e-92,   2.05134460e-92, ...,\n",
       "           2.05134460e-92,   2.05134460e-92,   2.05134460e-92],\n",
       "        [  1.22753508e-92,   1.22753508e-92,   1.22753508e-92, ...,\n",
       "           1.22753508e-92,   1.22753508e-92,   1.22753508e-92],\n",
       "        [  3.30537318e-93,   3.30537318e-93,   3.30537318e-93, ...,\n",
       "           3.30537318e-93,   3.30537318e-93,   3.30537318e-93]]),\n",
       " 'gamma_threshold': 0.001,\n",
       " 'id2word': <gensim.corpora.dictionary.Dictionary at 0x7f61f0e14e10>,\n",
       " 'iterations': 50,\n",
       " 'minimum_probability': 0,\n",
       " 'num_terms': 73672,\n",
       " 'num_topics': 200,\n",
       " 'num_updates': 189088L,\n",
       " 'numworkers': 1,\n",
       " 'offset': 1.0,\n",
       " 'optimize_alpha': False,\n",
       " 'optimize_eta': False,\n",
       " 'passes': 200,\n",
       " 'state': <gensim.models.ldamodel.LdaState at 0x7f618c89e850>,\n",
       " 'update_every': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the accessible variables in the LDA model structure\n",
    "lda.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out the internal help document for the LDA model class you can use the help() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2a72aece-5261-4bad-883b-b21455f957cc"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LdaModel in module gensim.models.ldamodel object:\n",
      "\n",
      "class LdaModel(gensim.interfaces.TransformationABC)\n",
      " |  The constructor estimates Latent Dirichlet Allocation model parameters based\n",
      " |  on a training corpus:\n",
      " |  \n",
      " |  >>> lda = LdaModel(corpus, num_topics=10)\n",
      " |  \n",
      " |  You can then infer topic distributions on new, unseen documents, with\n",
      " |  \n",
      " |  >>> doc_lda = lda[doc_bow]\n",
      " |  \n",
      " |  The model can be updated (trained) with new documents via\n",
      " |  \n",
      " |  >>> lda.update(other_corpus)\n",
      " |  \n",
      " |  Model persistency is achieved through its `load`/`save` methods.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LdaModel\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, bow, eps=None)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `eps`).\n",
      " |  \n",
      " |  __init__(self, corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01)\n",
      " |      If given, start training from the iterable `corpus` straight away. If not given,\n",
      " |      the model is left untrained (presumably because you want to call `update()` manually).\n",
      " |      \n",
      " |      `num_topics` is the number of requested latent topics to be extracted from\n",
      " |      the training corpus.\n",
      " |      \n",
      " |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      " |      used to determine the vocabulary size, as well as for debugging and topic\n",
      " |      printing.\n",
      " |      \n",
      " |      `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
      " |      (theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
      " |      1.0/num_topics prior.\n",
      " |      \n",
      " |      `alpha` can be set to an explicit array = prior of your choice. It also\n",
      " |      support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
      " |      normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
      " |      prior directly from your data.\n",
      " |      \n",
      " |      `eta` can be a scalar for a symmetric prior over topic/word\n",
      " |      distributions, or a matrix of shape num_topics x num_words, which can\n",
      " |      be used to impose asymmetric priors over the word distribution on a\n",
      " |      per-topic basis. This may be useful if you want to seed certain topics\n",
      " |      with particular words by boosting the priors for those words.  It also\n",
      " |      supports the special value 'auto', which learns an asymmetric prior\n",
      " |      directly from your data.\n",
      " |      \n",
      " |      Turn on `distributed` to force distributed computing (see the `web tutorial <http://radimrehurek.com/gensim/distributed.html>`_\n",
      " |      on how to set up a cluster of machines for gensim).\n",
      " |      \n",
      " |      Calculate and log perplexity estimate from the latest mini-batch every\n",
      " |      `eval_every` model updates (setting this to 1 slows down training ~2x;\n",
      " |      default is 10 for better performance). Set to None to disable perplexity estimation.\n",
      " |      \n",
      " |      `decay` and `offset` parameters are the same as Kappa and Tau_0 in\n",
      " |      Hoffman et al, respectively.\n",
      " |      \n",
      " |      `minimum_probability` controls filtering the topics returned for a document (bow).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=100)  # train model\n",
      " |      >>> print(lda[doc_bow]) # get topic probability distribution for a document\n",
      " |      >>> lda.update(corpus2) # update the LDA model with additional documents\n",
      " |      >>> print(lda[doc_bow])\n",
      " |      \n",
      " |      >>> lda = LdaModel(corpus, num_topics=50, alpha='auto', eval_every=5)  # train asymmetric alpha from data\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  bound(self, corpus, gamma=None, subsample_ratio=1.0)\n",
      " |      Estimate the variational bound of documents from `corpus`:\n",
      " |      E_q[log p(corpus)] - E_q[log q(corpus)]\n",
      " |      \n",
      " |      `gamma` are the variational parameters on topic weights for each `corpus`\n",
      " |      document (=2d matrix=what comes out of `inference()`).\n",
      " |      If not supplied, will be inferred from the model.\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear model state (free up some memory). Used in the distributed algo.\n",
      " |  \n",
      " |  do_estep(self, chunk, state=None)\n",
      " |      Perform inference on a chunk of documents, and accumulate the collected\n",
      " |      sufficient statistics in `state` (or `self.state` if None).\n",
      " |  \n",
      " |  do_mstep(self, rho, other, extra_pass=False)\n",
      " |      M step: use linear interpolation between the existing topics and\n",
      " |      collected sufficient statistics in `other` to update the topics.\n",
      " |  \n",
      " |  get_document_topics(self, bow, minimum_probability=None)\n",
      " |      Return topic distribution for the given document `bow`, as a list of\n",
      " |      (topic_id, topic_probability) 2-tuples.\n",
      " |      \n",
      " |      Ignore topics with very low probability (below `minimum_probability`).\n",
      " |  \n",
      " |  get_topic_terms(self, topicid, topn=10)\n",
      " |      Return a list of `(word_id, probability)` 2-tuples for the most\n",
      " |      probable words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  inference(self, chunk, collect_sstats=False)\n",
      " |      Given a chunk of sparse document vectors, estimate gamma (parameters\n",
      " |      controlling the topic weights) for each document in the chunk.\n",
      " |      \n",
      " |      This function does not modify the model (=is read-only aka const). The\n",
      " |      whole input chunk of document is assumed to fit in RAM; chunking of a\n",
      " |      large corpus must be done earlier in the pipeline.\n",
      " |      \n",
      " |      If `collect_sstats` is True, also collect sufficient statistics needed\n",
      " |      to update the model's topic-word distributions, and return a 2-tuple\n",
      " |      `(gamma, sstats)`. Otherwise, return `(gamma, None)`. `gamma` is of shape\n",
      " |      `len(chunk) x self.num_topics`.\n",
      " |      \n",
      " |      Avoids computing the `phi` variational parameter directly using the\n",
      " |      optimization presented in **Lee, Seung: Algorithms for non-negative matrix factorization, NIPS 2001**.\n",
      " |  \n",
      " |  init_dir_prior(self, prior, name)\n",
      " |  \n",
      " |  log_perplexity(self, chunk, total_docs=None)\n",
      " |      Calculate and return per-word likelihood bound, using the `chunk` of\n",
      " |      documents as evaluation corpus. Also output the calculated statistics. incl.\n",
      " |      perplexity=2^(-bound), to log at INFO level.\n",
      " |  \n",
      " |  print_topic(self, topicid, topn=10)\n",
      " |      Return the result of `show_topic`, but formatted as a single string.\n",
      " |  \n",
      " |  print_topics(self, num_topics=10, num_words=10)\n",
      " |  \n",
      " |  save(self, fname, ignore=['state', 'dispatcher'], *args, **kwargs)\n",
      " |      Save the model to file.\n",
      " |      \n",
      " |      Large internal arrays may be stored into separate files, with `fname` as prefix.\n",
      " |      \n",
      " |      `separately` can be used to define which arrays should be stored in separate files.\n",
      " |      \n",
      " |      `ignore` parameter can be used to define which variables should be ignored, i.e. left\n",
      " |      out from the pickled lda model. By default the internal `state` is ignored as it uses\n",
      " |      its own serialisation not the one provided by `LdaModel`. The `state` and `dispatcher\n",
      " |      will be added to any ignore parameter defined.\n",
      " |      \n",
      " |      \n",
      " |      Note: do not save as a compressed file if you intend to load the file back with `mmap`.\n",
      " |      \n",
      " |      Note: If you intend to use models across Python 2/3 versions there are a few things to\n",
      " |      keep in mind:\n",
      " |      \n",
      " |        1. The pickled Python dictionaries will not work across Python versions\n",
      " |        2. The `save` method does not automatically save all NumPy arrays using NumPy, only\n",
      " |           those ones that exceed `sep_limit` set in `gensim.utils.SaveLoad.save`. The main\n",
      " |           concern here is the `alpha` array if for instance using `alpha='auto'`.\n",
      " |      \n",
      " |      Please refer to the wiki recipes section (https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ#q9-how-do-i-load-a-model-in-python-3-that-was-trained-and-saved-using-python-2)\n",
      " |      for an example on how to work around these issues.\n",
      " |  \n",
      " |  show_topic(self, topicid, topn=10)\n",
      " |      Return a list of `(word, probability)` 2-tuples for the most probable\n",
      " |      words in topic `topicid`.\n",
      " |      \n",
      " |      Only return 2-tuples for the topn most probable words (ignore the rest).\n",
      " |  \n",
      " |  show_topics(self, num_topics=10, num_words=10, log=False, formatted=True)\n",
      " |      For `num_topics` number of topics, return `num_words` most significant words\n",
      " |      (10 words per topic, by default).\n",
      " |      \n",
      " |      The topics are returned as a list -- a list of strings if `formatted` is\n",
      " |      True, or a list of `(word, probability)` 2-tuples if False.\n",
      " |      \n",
      " |      If `log` is True, also output this result to log.\n",
      " |      \n",
      " |      Unlike LSA, there is no natural ordering between the topics in LDA.\n",
      " |      The returned `num_topics <= self.num_topics` subset of all topics is therefore\n",
      " |      arbitrary and may change between two LDA training runs.\n",
      " |  \n",
      " |  sync_state(self)\n",
      " |  \n",
      " |  top_topics(self, corpus, num_words=20)\n",
      " |      Calculate the Umass topic coherence for each topic. Algorithm from\n",
      " |      **Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011.**\n",
      " |  \n",
      " |  update(self, corpus, chunksize=None, decay=None, offset=None, passes=None, update_every=None, eval_every=None, iterations=None, gamma_threshold=None, chunks_as_numpy=False)\n",
      " |      Train the model with new documents, by EM-iterating over `corpus` until\n",
      " |      the topics converge (or until the maximum number of allowed iterations\n",
      " |      is reached). `corpus` must be an iterable (repeatable stream of documents),\n",
      " |      \n",
      " |      In distributed mode, the E step is distributed over a cluster of machines.\n",
      " |      \n",
      " |      This update also supports updating an already trained model (`self`)\n",
      " |      with new documents from `corpus`; the two models are then merged in\n",
      " |      proportion to the number of old vs. new documents. This feature is still\n",
      " |      experimental for non-stationary input streams.\n",
      " |      \n",
      " |      For stationary input (no topic drift in new documents), on the other hand,\n",
      " |      this equals the online update of Hoffman et al. and is guaranteed to\n",
      " |      converge for any `decay` in (0.5, 1.0>. Additionally, for smaller\n",
      " |      `corpus` sizes, an increasing `offset` may be beneficial (see\n",
      " |      Table 1 in Hoffman et al.)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ------------\n",
      " |      corpus: (gensim corpus object, list of tuples)\n",
      " |          The corpus with which the LDA model should be updated with.\n",
      " |      \n",
      " |      chunks_as_numpy: bool\n",
      " |          Whether each chunk passed to `.inference` should be a numpy\n",
      " |          array of not. Numpy can in some settings turn the term IDs\n",
      " |          into floats, these will be converted back into integers in\n",
      " |          inference, which incurs a performance hit. For distributed\n",
      " |          computing it may be desirable to keep the chunks as numpy\n",
      " |          arrays.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      For other parameter settings see LdaModel().\n",
      " |  \n",
      " |  update_alpha(self, gammat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-document\n",
      " |      topic weights `alpha` given the last `gammat`.\n",
      " |  \n",
      " |  update_eta(self, lambdat, rho)\n",
      " |      Update parameters for the Dirichlet prior on the per-topic\n",
      " |      word weights `eta` given the last `lambdat`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(cls, fname, *args, **kwargs) from __builtin__.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      Large arrays can be memmap'ed back as read-only (shared memory) by setting `mmap='r'`:\n",
      " |      \n",
      " |          >>> LdaModel.load(fname, mmap='r')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To print the help document for the LDA model class\n",
    "help(lda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print_topics(n) method of the LDA model object prints out a random sampling of n different learned topics as represented by the most likely terms in the topic's langauge model, i.e. the terms that maximize the topic language model P(term|topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "98a57c6a-25fc-4597-9098-5521b9153f5b"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(99,\n",
       "  u'0.132*fund + 0.107*pay + 0.029*settlement + 0.028*resulting + 0.021*disclose + 0.020*requires + 0.020*losses + 0.019*tobacco + 0.017*result + 0.015*caused'),\n",
       " (36,\n",
       "  u'0.279*senate + 0.068*time + 0.058*house + 0.038*house_of_representatives + 0.027*declaration + 0.021*designate + 0.020*interpretation + 0.018*cloture + 0.017*south + 0.015*east'),\n",
       " (137,\n",
       "  u'0.106*secretary + 0.065*program + 0.050*technology + 0.033*activities + 0.020*research + 0.019*development + 0.017*conduct + 0.016*carry + 0.014*transfer + 0.013*identify'),\n",
       " (85,\n",
       "  u'0.080*imposes + 0.078*article + 0.073*hours + 0.058*authorizing + 0.047*long-term_care + 0.046*awareness_week + 0.029*applying + 0.025*william + 0.022*hispanic + 0.022*leader'),\n",
       " (145,\n",
       "  u'0.075*purchase + 0.039*program + 0.035*importation + 0.034*finance + 0.030*purchases + 0.027*commodities + 0.021*purchased + 0.018*crops + 0.018*donations + 0.017*courses'),\n",
       " (45,\n",
       "  u'0.467*national + 0.154*day + 0.062*encourages + 0.026*americans + 0.025*honor + 0.013*kansas + 0.013*celebration + 0.012*designates_september + 0.011*designates_april + 0.010*store'),\n",
       " (150,\n",
       "  u'0.148*production + 0.057*victim + 0.046*allowance + 0.043*discrimination + 0.042*reductions + 0.034*leasing + 0.030*producer + 0.028*abroad + 0.027*grounds + 0.024*respecting'),\n",
       " (151,\n",
       "  u'0.253*property + 0.066*september_30 + 0.062*vehicle + 0.037*real_property + 0.035*city + 0.032*owners + 0.029*residents + 0.027*properties + 0.024*sharing + 0.020*motor_vehicles'),\n",
       " (165,\n",
       "  u'0.107*administration + 0.100*annually + 0.055*transaction + 0.050*separate + 0.045*obligations + 0.037*applied + 0.035*obtaining + 0.028*panel + 0.026*aviation + 0.022*denies'),\n",
       " (33,\n",
       "  u'0.151*construction + 0.116*costs + 0.050*planning + 0.032*documents + 0.032*restoration + 0.023*content + 0.022*buildings + 0.019*20_percent + 0.018*imposing + 0.018*firms')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6975bd59-2a7c-466c-b248-7dbff82dea79"
    }
   },
   "source": [
    "#### Infer the Document Probability Score P(topic|doc) using the LDA Model\n",
    "\n",
    "In this section, each document from the corpus is passed into the LDA model which then infers the topic distribution for each document. The topic distributions are collected into a single numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fe638683-0efb-49b6-93d2-4e8f22f15e6a"
    }
   },
   "outputs": [],
   "source": [
    "# To retrieve all topics and their probabilities we must set the LDA minimum probability setting to zero\n",
    "lda.minimum_probability = 0\n",
    "\n",
    "# This function generates the topic probabilities for each doc from the trained LDA model\n",
    "# The probabilities go into a single matrix where the rows are documents and columns are topics\n",
    "def ExtractDocTopicProbsMatrix(corpus,lda):\n",
    "    # Initialize the matrix\n",
    "    docTopicProbs = numpy.zeros((len(corpus),lda.num_topics))\n",
    "    for docID in range(0,len(corpus)):\n",
    "        for topicProb in lda[corpus[docID]]:\n",
    "            docTopicProbs[docID,topicProb[0]]=topicProb[1]\n",
    "    return docTopicProbs\n",
    "\n",
    "# docTopicProbs[docID,TopicID] --> P(topic|doc)\n",
    "if False:\n",
    "    docTopicProbs = ExtractDocTopicProbsMatrix(corpus,lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    numpy.save(\"../Data/CongressionalDocTopicProbs.npy\",docTopicProbs)\n",
    "    \n",
    "if True:\n",
    "    docTopicProbs = numpy.load(\"../Data/CongressionalDocTopicProbs.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "65904084-4721-4b58-8ea5-711b7b6388fa"
    }
   },
   "source": [
    "#### Compute the Global Topic Likelihood Scores P(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b43ca7bd-75e7-4d17-9f77-80ce05375dad"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the global topic likelihoods by aggregating topic probabilities over all documents\n",
    "# topicProbs[topicID] --> P(topic)\n",
    "def ComputeTopicProbs(docTopicProbs):\n",
    "    topicProbs = docTopicProbs.sum(axis=0) \n",
    "    topicProbs = topicProbs/sum(topicProbs)\n",
    "    return topicProbs\n",
    "\n",
    "topicProbs = ComputeTopicProbs(docTopicProbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "82e2d906-b067-419d-9532-378c6f36fd69"
    }
   },
   "source": [
    "#### Convert the Topic Language Model Information P(term|topic) from the LDA Model into a NumPy Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7fd9e75d-7abf-4dca-b173-23405ebdfbf6"
    }
   },
   "outputs": [],
   "source": [
    "def ExtractTopicLMMatrix(lda):\n",
    "    # Initialize the matrix\n",
    "    docTopicProbs = numpy.zeros((lda.num_topics,lda.num_terms))\n",
    "    for topicID in range(0,lda.num_topics):\n",
    "        termProbsList = lda.get_topic_terms(topicID,lda.num_terms)\n",
    "        for termProb in termProbsList:\n",
    "            docTopicProbs[topicID,termProb[0]]=termProb[1]\n",
    "    return docTopicProbs\n",
    "    \n",
    "# topicTermProbs[topicID,termID] --> P(term|topic)\n",
    "topicTermProbs = ExtractTopicLMMatrix(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    numpy.save(\"../Data/CongressionalDocTopicLM.npy\",topicTermProbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "688b7bd8-bb00-442d-a195-dd1310424d82"
    }
   },
   "source": [
    "#### Compute P(topic,term), P(term), and P(topic|term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ac3b8bc5-6733-46cf-9451-10b7c9ef8d88"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the joint likelihoods of topics and terms\n",
    "# jointTopicTermProbs[topicID,termID] --> P(topic,term) = P(term|topic)*P(topic)\n",
    "jointTopicTermProbs = numpy.diag(topicProbs).dot(topicTermProbs) \n",
    "\n",
    "# termProbs[termID] --> P(term)\n",
    "termProbs = jointTopicTermProbs.sum(axis=0)\n",
    "\n",
    "# topicProbsPermTerm[topicID,termID] --> P(topic|term)\n",
    "topicProbsPerTerm = jointTopicTermProbs / termProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e45a357e-ab67-4d85-80e9-d433714f9798"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: requires --> 0.004203\n",
      "2: effective --> 0.004191\n",
      "3: amends --> 0.004003\n",
      "4: benefit --> 0.003621\n",
      "5: certain --> 0.002886\n",
      "6: including --> 0.002823\n",
      "7: program --> 0.002773\n",
      "8: harmonized_tariff_schedule_of_the_united_states --> 0.002734\n",
      "9: provide --> 0.002722\n",
      "10: united_states --> 0.002686\n",
      "11: july_7 --> 0.002637\n",
      "12: december_31 --> 0.002452\n",
      "13: all --> 0.002003\n",
      "14: prohibits --> 0.001951\n",
      "15: specified --> 0.001835\n",
      "16: individual --> 0.001796\n",
      "17: service --> 0.001726\n",
      "18: authorizes --> 0.001716\n",
      "19: programs --> 0.001673\n",
      "20: respect --> 0.001658\n",
      "21: states --> 0.001643\n",
      "22: services --> 0.001610\n",
      "23: congress --> 0.001530\n",
      "24: federal --> 0.001420\n",
      "25: senate --> 0.001419\n"
     ]
    }
   ],
   "source": [
    "# Print most frequent words in LDA vocab\n",
    "mostFrequentTermIDs = (-termProbs).argsort()\n",
    "for i in range(0,25):\n",
    "    print (\"%d: %s --> %f\" % (i+1, id2token[mostFrequentTermIDs[i]], termProbs[mostFrequentTermIDs[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9c6d2ec8-2fd6-43ac-89e7-293228019bb2"
    }
   },
   "source": [
    "#### Compute WPMI\n",
    "\n",
    "To determine which vocabulary terms are most representative of a topic, systems typically just choose a set of terms that are most likely for the topic, i.e., terms that maximize the languauge model expression <i>P(term|topic)</i> for the given topic. This approach is adequate for many data sets. However, for some data sets there may be common words in the corpus that are frequent terms within multiple topics, and hence not a distinguishing term for any of these topics. In this case, selecting words which have the largest weighted pointwise mutual information (WPMI) with a given topic is more appropriate. \n",
    "\n",
    "The expression for WPMI between a word and token is given as:\n",
    "\n",
    "\n",
    "$WPMI(term,topic) = P(term,topic)\\log\\frac{P(term,topic)}{P(term)P(topic)} = P(term,topic)\\log\\frac{P(topic|term)}{P(topic)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 73672)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicTermWPMI =(jointTopicTermProbs.transpose()*numpy.log(topicProbsPerTerm.transpose()/topicProbs)).transpose()\n",
    "topicTermWPMI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fd3d5300-60a1-4873-a52d-a9d5b89b3463"
    }
   },
   "source": [
    "#### Compute Topic to Document Purity measure for Each Topic\n",
    "\n",
    "One measure of the importance or quality of a topic is its topic to document purity measure. This purity measure assumes latent topics that dominate the documents in which they appear are more semantically important than latent topics that are weakly spread across many documents. This concept was introduced in the paper [\"Latent Topic Modeling for Audio Corpus Summarization](http://people.csail.mit.edu/hazen/publications/Hazen-Interspeech11.pdf). The purity measure is expressed by the following equation:\n",
    "\n",
    "$Purity(topic) = \\exp\\left (\n",
    "                 \\frac{\\sum_{\\forall doc}P(topic|doc)\\log P(topic|doc)}{\\sum_{\\forall doc}P(topic|doc)}\n",
    "                \\right )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f2b0a490-a67d-47fc-872f-1e4696886ecb"
    }
   },
   "outputs": [],
   "source": [
    "topicPurity = numpy.exp(((docTopicProbs * numpy.log(docTopicProbs)).sum(axis=0))/(docTopicProbs).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61921e49-660e-4d97-a4c8-feec4804e2ce"
    }
   },
   "source": [
    "#### Create Topic Summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CreateTermIDToSurfaceFormMapping(id2token,token2surfaceform):\n",
    "    termIDToSurfaceFormMap = []\n",
    "    for i in range(0,len(id2token)):\n",
    "        termIDToSurfaceFormMap.append(token2surfaceform[id2token[i]])\n",
    "    return termIDToSurfaceFormMap;\n",
    "\n",
    "termIDToSurfaceFormMap = CreateTermIDToSurfaceFormMapping(id2token, vocabToSurfaceFormHash);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobby Bobby\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "print(id2token[i], termIDToSurfaceFormMap[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "In the code snippet below we demonstrate how the WPMI measure lowers to score of some common token that do not provide value in a topic summary in comparison to the standard word likely measure P(token|topic). For topic 11 below notice how the generic words <i>certain</i>, <i>require</i>, <i>respect</i> and <i>provide</i> have their position in the summaries lowered by the WPMI measure.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "902f0953-1cc3-465a-a93b-aa08daf2b18b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        WPMI                                                 Prob\n",
      " 1:                              Amends ---> 0.017044                                 Amends ---> 0.282902\n",
      " 2:                            employer ---> 0.002314                               employer ---> 0.040485\n",
      " 3:                             section ---> 0.001934                                section ---> 0.032095\n",
      " 4:                                   B ---> 0.001661                                      B ---> 0.027566\n",
      " 5:            Authorizes the President ---> 0.001564               Authorizes the President ---> 0.025961\n",
      " 6:                              Alaska ---> 0.001473                                 Alaska ---> 0.024452\n",
      " 7:                              revise ---> 0.001079                                certain ---> 0.022691\n",
      " 8:                       Miscellaneous ---> 0.000923                                 revise ---> 0.019850\n",
      " 9:                             Extends ---> 0.000920                                Extends ---> 0.019370\n",
      "10:                      Assistance Act ---> 0.000737                          Miscellaneous ---> 0.015696\n",
      "11:                           preceding ---> 0.000697                         Assistance Act ---> 0.012435\n",
      "12:                             certain ---> 0.000662                              preceding ---> 0.011768\n",
      "13:                             periods ---> 0.000542                                require ---> 0.010843\n",
      "14:                           allotment ---> 0.000472                                periods ---> 0.010177\n",
      "15:                          30 percent ---> 0.000422                              specified ---> 0.009797\n",
      "16:                           title III ---> 0.000382                               purposes ---> 0.008491\n",
      "17:                             require ---> 0.000368                              allotment ---> 0.008231\n",
      "18:                           incentive ---> 0.000361                               Includes ---> 0.007595\n",
      "19:                         description ---> 0.000341                                respect ---> 0.007314\n",
      "20:                            monetary ---> 0.000339                             30 percent ---> 0.007011\n",
      "21:             administrative expenses ---> 0.000283                            description ---> 0.006564\n",
      "22:                           modifying ---> 0.000281                               provided ---> 0.006538\n",
      "23:                            Includes ---> 0.000268                              title III ---> 0.006337\n",
      "24:                         accompanied ---> 0.000265                              incentive ---> 0.006292\n",
      "25:                       Act to extend ---> 0.000258                                provide ---> 0.006171\n"
     ]
    }
   ],
   "source": [
    "topicID = 11\n",
    "highestWPMITermIDs = (-topicTermWPMI[topicID]).argsort()\n",
    "highestProbTermIDs = (-topicTermProbs[topicID]).argsort()\n",
    "print (\"                                        WPMI                                                 Prob\")\n",
    "for i in range(0,25):\n",
    "    print (\"%2d: %35s ---> %8.6f    %35s ---> %8.6f\" % (i+1, \n",
    "                                                        termIDToSurfaceFormMap[highestWPMITermIDs[i]], \n",
    "                                                        topicTermWPMI[topicID,highestWPMITermIDs[i]],\n",
    "                                                        termIDToSurfaceFormMap[highestProbTermIDs[i]], \n",
    "                                                        topicTermProbs[topicID,highestProbTermIDs[i]]))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5fef2737-5cde-43e2-8c05-04320722da93"
    }
   },
   "outputs": [],
   "source": [
    "def CreateTopicSummaries(topicTermScores, id2token, tokenid2surfaceform, maxStringLen):\n",
    "    topicSummaries = []\n",
    "    for topicID in range(0,len(topicTermScores)):\n",
    "        rankedTermIDs = (-topicTermScores[topicID]).argsort()\n",
    "        maxNumTerms = len(rankedTermIDs)\n",
    "        termIndex = 0\n",
    "        stop = 0\n",
    "        outputTokens = []\n",
    "        topicSummary = \"\"\n",
    "        while not stop:\n",
    "            # If we've run out of tokens then stop...\n",
    "            if (termIndex>=maxNumTerms):\n",
    "                stop=1\n",
    "            # ...otherwise consider adding next token to summary\n",
    "            else:\n",
    "                nextToken = id2token[rankedTermIDs[termIndex]]\n",
    "                nextTokenOut = tokenid2surfaceform[rankedTermIDs[termIndex]]\n",
    "                keepToken = 1\n",
    "                # See if we should ignore this token\n",
    "                if len(outputTokens) > 0:\n",
    "                    for prevToken in outputTokens:\n",
    "                        # Ignore token if it is a substring of a previous token\n",
    "                        if nextToken in prevToken:\n",
    "                            keepToken = 0\n",
    "                            break\n",
    "                        # Ignore token if it is a superstring of a previous token\n",
    "                        elif prevToken in nextToken:\n",
    "                            keepToken = 0\n",
    "                            break\n",
    "                if keepToken:\n",
    "                    # Always add at least one token to the summary\n",
    "                    if len(topicSummary) == 0:\n",
    "                        topicSummary = nextTokenOut\n",
    "                        outputTokens.append(nextToken)\n",
    "                    # Add additional tokens if the summary string length is less than maxStringLen\n",
    "                    elif ( len(topicSummary) + len(nextTokenOut) + 2 < maxStringLen):\n",
    "                        topicSummary += \", \" + nextTokenOut\n",
    "                        outputTokens.append(nextToken)\n",
    "                    else:\n",
    "                        stop=1\n",
    "                termIndex += 1         \n",
    "        topicSummaries.append(topicSummary)\n",
    "    return topicSummaries   \n",
    "    \n",
    "topicSummaries = CreateTopicSummaries(topicTermWPMI, id2token, termIDToSurfaceFormMap, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bbe5cc20-ad4f-482c-8f94-09de87752d30"
    }
   },
   "outputs": [],
   "source": [
    "# Rank the topics by their prominence score in the corpus\n",
    "# The topic score combines the total weight of each a topic in the corpus \n",
    "# with a topic document purity score for topic \n",
    "# Topics with topicScore > 1 are generally very strong strong topics\n",
    "\n",
    "topicScore =  (numTopics * topicProbs) * (2 * topicPurity)\n",
    "topicRanking = (-topicScore).argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save LDA Topic Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "dacd12f0-444b-4759-af58-27d4ba1c4fe5"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  Score  Prob  Purity  Summary\n",
      "  0 180  2.293 (2.031, 0.282) Harmonized Tariff Schedule of the United States, December 31, pilot program\n",
      "  1 135  0.699 (1.228, 0.142) interest, deduction, Pine, Amends the Internal Revenue Code, distributions\n",
      "  2   1  0.688 (0.741, 0.232) regard, minor, Postal Service, United States for permanent, lawfully admitted\n",
      "  3  11  0.491 (1.415, 0.087) Amends, employer, section, B, Authorizes the President, Alaska, revise\n",
      "  4 178  0.467 (1.311, 0.089) insurance, percent, Trust Fund, SSA, accounts, increase, amounts, adjustment\n",
      "  5  45  0.408 (0.844, 0.121) effective, day, Encourages, Americans, honor, Kansas, celebration\n",
      "  6 170  0.400 (0.800, 0.125) covered, State, Applies, formula, Social Security, Old Age\n",
      "  7 111  0.387 (0.803, 0.120) Medicare, Social Security Act, physician, services, individuals, Medicaid\n",
      "  8  90  0.339 (0.933, 0.091) Recognizes, people, Calls, Urges, efforts, human rights, supports, political\n",
      "  9  67  0.326 (0.835, 0.098) emissions, management plan, waters, users, Pennsylvania, railroad, boundary\n",
      " 10  27  0.316 (0.946, 0.084) liability, District, extend, Park, farm, producers, specified, Agriculture\n",
      " 11  59  0.312 (1.154, 0.068) provisions, July 7, Revises, Expresses the sense of the Senate, certain, Allows\n",
      " 12  82  0.304 (0.897, 0.085) assistance, Calls on the government, support, Commends, United Nations, governments\n",
      " 13  51  0.272 (0.727, 0.093) legislation, budget, resolution, Congress, spending, obligated, outlays\n",
      " 14  54  0.240 (0.936, 0.064) program, education, youth, partnerships, funds, improve, participation, Includes\n",
      " 15 149  0.237 (0.910, 0.065) employees, employment, expenses, EPA, employed, offenses, aliens, LEA, prohibited\n",
      " 16 199  0.235 (1.091, 0.054) requirements, Requires, persons, determining, rates, standards, reasonable, required\n",
      " 17  52  0.234 (0.738, 0.079) Commission, Congress, committees, NATO membership criteria, staff, majority, submit\n",
      " 18 175  0.230 (0.898, 0.064) FY, aircraft, Increases, amounts, total, program, Mandates, cost, funds, Requires\n",
      " 19  65  0.227 (0.712, 0.080) duty, vessel, oil, veteran, juvenile, excise tax, impacts, diabetes, ethanol\n",
      " 20  93  0.224 (0.937, 0.060) community, Administrator, Director, establish, develop, communities, implement\n",
      " 21 181  0.212 (0.974, 0.054) study, review, enactment of this Act, Council, regulation, Establishes, policies\n",
      " 22 105  0.202 (0.541, 0.093) located, Internet, substances, power, Street, charged, digital, Kentucky, send, bear\n",
      " 23  69  0.200 (0.679, 0.074) credit, claim, small business, qualifying, SBA, Virginia, IRS, eligible\n",
      " 24 173  0.195 (0.636, 0.077) coverage, Amends title XVIII, conveyance, natural, Governor, Federal employees\n",
      " 25  88  0.192 (0.574, 0.084) students, school, teachers, Requires, IHEs, HEA, geological\n",
      " 26  89  0.190 (0.903, 0.053) report, compliance, procedures, effect, Requires, requirement, schedule, filing\n",
      " 27 166  0.187 (0.793, 0.059) grants, grade, pipeline, selling, awarding, including, Federal share\n",
      " 28  62  0.184 (0.690, 0.067) funds, appropriations, Prohibits, available, Limits, programs, funded, activities\n",
      " 29 131  0.182 (0.782, 0.058) Authorizes appropriations, project, Requires the Secretary, design, site, portion\n",
      " 30  61  0.182 (0.652, 0.070) energy, facilities, vehicles, fuel, Oregon, gas, spectrum, Amtrak, periodic\n",
      " 31  13  0.181 (0.699, 0.065) treatment, health, medical, prevention, disease, patient, outreach, physicians\n",
      " 32   3  0.180 (0.557, 0.081) children, parents, child support, rail, annuity, residence, custody, child's, North\n",
      " 33  24  0.179 (0.452, 0.099) Week, specified provisions, television, older individuals, radon, unpaid\n",
      " 34 190  0.178 (0.803, 0.055) United States, U.S., country, international, invited, negotiations, global\n",
      " 35 107  0.169 (0.879, 0.048) subject, Makes, pursuant, Specifies, sales, Permits, restrictions, extent, acquire\n",
      " 36 184  0.165 (0.471, 0.088) imports, merchandise, trade, Japan, certain, investigation, Island\n",
      " 37  37  0.164 (0.500, 0.082) Wilderness, convey, University, Trail, English, mining, appreciation, culture\n",
      " 38 194  0.148 (0.551, 0.067) Designates, sites, uses, cultural, conduct a study, plant, recreational, addition\n",
      " 39  36  0.148 (0.509, 0.073) Senate, time, House, declaration, interpretation, cloture, designate, South, East\n",
      " 40 193  0.147 (0.570, 0.065) tax, income, derived, size, cost-effective, 20 years, surplus, exclude, accessible\n",
      " 41   6  0.146 (0.767, 0.048) benefit, States, eligible, election, candidate, Requires each State, four years\n",
      " 42  66  0.145 (0.666, 0.055) agency, agencies, Federal, rule, tribal, Indian, functions, oversight, regulatory\n",
      " 43  43  0.145 (0.598, 0.061) NATO membership criteria, military, license, armed forces, personnel, reserve\n",
      " 44  56  0.143 (0.478, 0.075) veterans, VA, Federal lands, historical, educational assistance, parts\n",
      " 45  44  0.138 (0.523, 0.066) consumer, suspend, enforcement, discount card, documentation, labeling, outside\n",
      " 46 168  0.126 (0.506, 0.062) County, California, New Mexico, New York, associated, National Park, allocate, Fort\n",
      " 47 137  0.125 (0.667, 0.047) Secretary, technology, program, activities, carry, development, research, conduct\n",
      " 48  39  0.125 (0.427, 0.073) anniversary, Secretary of the Treasury, judgment, named, Missouri, coins\n",
      " 49 122  0.124 (0.488, 0.064) water, chemical, sources, management, regulated, fish and wildlife, Ohio, delivered\n",
      " 50 119  0.123 (0.406, 0.076) month, units, treat, importance, alcohol, wage, card, drug abuse, evaluating\n",
      " 51   9  0.122 (0.504, 0.061) foreign, practice, lead, involved, participants, sold, involving, illegal\n",
      " 52 116  0.122 (0.460, 0.066) contributions, receipt, recognition, Library, special pay, recognized, remains\n",
      " 53 117  0.120 (0.413, 0.073) bank, Tribe, securities, debtor, FDIC, issuer, South Florida, depository institution\n",
      " 54  99  0.118 (0.613, 0.048) Fund, pay, settlement, resulting, disclose, losses, tobacco, caused\n",
      " 55 123  0.116 (0.632, 0.046) action, order, party, affected, parties, conduct, administrative, legal, intent\n",
      " 56 174  0.116 (0.537, 0.054) victims, law enforcement, violations, prosecution, Justice, crimes, committed\n",
      " 57  72  0.115 (0.434, 0.066) citizens, ten years, visa, judge, valid, South Africa, Mexico, concern\n",
      " 58 146  0.109 (0.556, 0.049) DOD, Corporation, performance, Federal agency, award, receives, authorized, Requires\n",
      " 59  40  0.109 (0.590, 0.046) development, funding, retain, Initiative, Indian tribes, monitoring, advanced\n",
      " 60 160  0.109 (0.466, 0.058) research, technologies, NIH, preservation, cancer, Healthy\n",
      " 61   2  0.107 (0.555, 0.048) allow, tax credit, Modifies, possession, Convention, capacity, preparation\n",
      " 62 106  0.106 (0.556, 0.048) information, records, intelligence, Requires, identification, dissemination\n",
      " 63 196  0.106 (0.620, 0.043) benefits, data, eligibility, emergency, businesses, premium, processes, costs\n",
      " 64 144  0.104 (0.513, 0.051) plan, rules, personal, retirement, offer, disabled, ERISA, statewide, modernization\n",
      " 65  31  0.103 (0.485, 0.053) land, National Forest, Secretary, title, Prescribes, describing, Amends the Omnibus\n",
      " 66  75  0.103 (0.470, 0.055) HHS, Medicaid, provider, Secretary of Health and Human Services, applicants\n",
      " 67 133  0.102 (0.502, 0.051) care, lease, providers, hospitals, patients, previous, medical, professionals\n",
      " 68 120  0.101 (0.502, 0.050) service, entered, life, volunteers, Native American, placed, display, affiliate\n",
      " 69 172  0.098 (0.467, 0.053) security, DHS, terrorism, current law, terrorist, attacks, Harbor, techniques, DNI\n",
      " 70  23  0.098 (0.433, 0.057) drug, spouse, foreign officials, establish a program\n",
      " 71 125  0.098 (0.470, 0.052) Attorney General, alien, application, investigation, DOJ, detention, FBI, licenses\n",
      " 72  46  0.098 (0.418, 0.058) lands, reservation, mineral, United States, nomination, terminating, Jr, all, A.\n",
      " 73  15  0.097 (0.565, 0.043) expenditures, value, excess, amendment, limit, equal, aggregate, waiver, source\n",
      " 74 145  0.096 (0.423, 0.057) purchase, importation, finance, commodities, program, courses, donations, crops\n",
      " 75 192  0.096 (0.422, 0.057) requirement, enrollment, enrolled, manufacture, measure, payable, joint resolution\n",
      " 76  20  0.095 (0.466, 0.051) distribution, food, record, materials, inventory, Senators\n",
      " 77 163  0.094 (0.506, 0.046) person, penalties, penalty, knowingly, inspection, transmission, operators\n",
      " 78  96  0.090 (0.417, 0.054) conservation, resources, species, coastal, managed, Corridor, ocean, marine, transit\n",
      " 79   8  0.090 (0.488, 0.046) business, equipment, fraud, audit, public health, veterans', team, field, efficiency\n",
      " 80 140  0.089 (0.405, 0.055) loan, mortgage, Extends, exposure, borrower, et al, insured, World War II, lending\n",
      " 81 147  0.089 (0.439, 0.051) certified, 50 percent, reduction, ten percent, technical, articles, nutrition\n",
      " 82  10  0.089 (0.557, 0.040) currently, minimum, having, hospital, three years, discharge, strength, present\n",
      " 83 114  0.089 (0.358, 0.062) account, 90 days, people of the United States, person's, statute, animals, visit\n",
      " 84 179  0.089 (0.412, 0.054) housing, defendant, requested, class, homeless, testimony, acquire lands\n",
      " 85  42  0.089 (0.594, 0.037) access, exempt, improving, device, factors, barriers, examination, means\n",
      " 86  76  0.088 (0.398, 0.055) science, space, recycling, mathematics, America, Nation's, mission, 100th\n",
      " 87 159  0.087 (0.441, 0.050) qualified, notice, District of Columbia, patent, electronic, incorporated, trademark\n",
      " 88  38  0.087 (0.514, 0.043) President, Expresses the sense of the Congress, official, major\n",
      " 89  60  0.087 (0.572, 0.038) Directs the Secretary, grant, annual, applications, population, training activities\n",
      " 90 113  0.085 (0.382, 0.056) exceed, taking place, determined, Expresses support, economic, Affairs, debate\n",
      " 91  47  0.084 (0.557, 0.038) management, systems, organization, response, facilities, nonprofit, commodity\n",
      " 92  71  0.084 (0.429, 0.049) products, export, goods, China, inspections, produce, customs, expiration, prisoners\n",
      " 93   7  0.083 (0.391, 0.053) River, LEAs, local educational agencies, Utah, classified, demonstrate, Idaho\n",
      " 94  63  0.083 (0.430, 0.048) local, fee, Lake, instruction, international organizations and programs\n",
      " 95 151  0.082 (0.459, 0.045) property, September 30, vehicle, city, owners, residents, properties, sharing\n",
      " 96 183  0.082 (0.496, 0.041) payments, training, demonstration, counseling, Coast Guard, small, clinical\n",
      " 97  19  0.081 (0.371, 0.055) repeal, inclusion, beneficiaries, March 1, enacted, point, New Jersey, methodology\n",
      " 98 162  0.081 (0.485, 0.042) member, Senator, decision, transactions, v., case, file, substantial, operated\n",
      " 99 154  0.081 (0.511, 0.040) services, number, single, receiving, Eliminates, Requires, consultation, selection\n",
      "100  18  0.081 (0.393, 0.052) offense, crime, relief, release, bond, convicted, commit, conviction, Commemorates\n",
      "101   5  0.081 (0.448, 0.045) individual, domestic, ITC, Requires the Attorney General, import relief, 21st\n",
      "102 177  0.080 (0.450, 0.045) authority, Federal Government, limited, Judiciary, information technology, studies\n",
      "103 188  0.079 (0.424, 0.047) American, foreign country, Washington, construct, Federal funds, subsidy\n",
      "104 118  0.078 (0.490, 0.040) transportation, limitations, agricultural, Improvement Act, contractors, share\n",
      "105  49  0.077 (0.545, 0.035) Prohibits, Allows, authorized, prohibition, solely, motor vehicle, elect\n",
      "106 108  0.076 (0.470, 0.041) acquisition, procurement, required, commerce, replacement, purchase, prior\n",
      "107  48  0.076 (0.389, 0.049) firearms, stock, SEC, Guam, disaster, bankruptcy, customer, Basin, fisheries\n",
      "108  77  0.075 (0.369, 0.051) U.S. officials, cases, Louisiana, judges, district court, attorneys, Creates, taking\n",
      "109 195  0.075 (0.523, 0.036) certification, list, position, group, serve, duties, behalf, appointed, offered\n",
      "110 169  0.075 (0.359, 0.052) requiring, systematically, set, consider, human, Constitutional Amendment\n",
      "111  55  0.075 (0.461, 0.040) rural, improvement, positions, priority, goals, innovation, financial institutions\n",
      "112  22  0.072 (0.510, 0.035) period, case, connection, special, authorization, Treasury, circumstances, overseas\n",
      "113  12  0.072 (0.437, 0.041) educational, world, Reform, leadership, address, principles, capabilities, threats\n",
      "114 185  0.072 (0.491, 0.037) product, issued, issuance, flood control, criminal, arising, evidence\n",
      "115 156  0.068 (0.465, 0.037) Limits, Permits, specific, Outlines, FCC, highway, prohibiting, attend\n",
      "116  26  0.066 (0.390, 0.043) collection, entitled, hearing, imposed, form, publication, collected, counsel\n",
      "117 141  0.064 (0.359, 0.045) officers, Nevada, NOAA, survey, acting through the Director, National Park Service\n",
      "118  87  0.063 (0.431, 0.037) transfer, investment, financial, assets, business, companies, jobs, enterprise\n",
      "119 198  0.063 (0.375, 0.042) agreement, Soviet Union, nuclear, presidential, enter, offices, Provides, Montana\n",
      "120  32  0.063 (0.434, 0.036) contract, disclosure, statement, interstate, cover, carrier, telecommunications\n",
      "121  33  0.063 (0.436, 0.036) construction, costs, planning, documents, restoration, content, buildings\n",
      "122 142  0.062 (0.369, 0.042) environmental, infrastructure, first, Institute, degree, administer, five, head\n",
      "123  64  0.062 (0.427, 0.036) rights, status, petition, condition, exercise, agreements, Texas, denial, things\n",
      "124  35  0.061 (0.397, 0.039) area, designation, company, manufacturing, corporations, consortium, Alabama\n",
      "125  28  0.061 (0.412, 0.037) analysis, potential, disposal, storage, waste, exchange, war, proposed, border\n",
      "126  86  0.061 (0.419, 0.036) Defines, women, related, physical, change, Indians, Georgia, living, addressing\n",
      "127 121  0.060 (0.368, 0.041) Center, maintenance, building, history, leave, threshold, cooperative, admission\n",
      "128  17  0.059 (0.388, 0.038) claims, rate, paid, Protection Act, pension, nurse, native, elected, regardless\n",
      "129 124  0.059 (0.401, 0.036) regulations, approved, applicable, dependents, accordance, enable, contained\n",
      "130 176  0.058 (0.427, 0.034) payment, percentage, years, Requires the Secretary, maintained, academic, cash\n",
      "131  74  0.058 (0.475, 0.031) Authorizes the Secretary, compensation, request, reporting\n",
      "132 104  0.058 (0.420, 0.034) taxpayer, identify, assist, administered, option, economic development\n",
      "133   0  0.057 (0.360, 0.040) apply, terminate, open, Museum, withdrawal, forces, long, secure, cause, Afghanistan\n",
      "134  16  0.057 (0.399, 0.036) provision, Department of Veterans Affairs, effects, prepare, job, impact\n",
      "135  57  0.057 (0.395, 0.036) permit, nonprofit organizations, Secretary of Agriculture, listed, approving, asset\n",
      "136 187  0.057 (0.415, 0.034) year, evaluation, injury, quality, average, carrying, computer, causes, Adds\n",
      "137 191  0.057 (0.397, 0.036) standards, institution, definition, association, hazardous waste, restrict, incident\n",
      "138 171  0.057 (0.346, 0.041) loans, debt, Navy, obligation, investments, interest rate, traffic, loan guarantees\n",
      "139  95  0.057 (0.391, 0.036) Exempts, reduce, removal, level, measures, mitigation, Lists, reduction, good\n",
      "140  91  0.056 (0.415, 0.034) Office, network, initial, capital, proceeds, operating, selected, six, particular\n",
      "141  98  0.056 (0.402, 0.035) date, public, private, registration, death, registered, religious, line\n",
      "142 134  0.055 (0.315, 0.044) Congratulates, participant, continued, arrangement, adverse, II, road, livestock\n",
      "143  78  0.055 (0.338, 0.041) training, Air Force, volunteer, Task Force, administering, developing countries\n",
      "144 164  0.055 (0.393, 0.035) activities, prohibit, activity, Corps, engage, candidates, independent\n",
      "145 148  0.055 (0.386, 0.036) policy, Department, promotion, transfers, authorities, train, weapons, active\n",
      "146 153  0.055 (0.336, 0.041) families, family, child care, allotments, State plan, substitute, medical assistance\n",
      "147 130  0.054 (0.421, 0.032) contracts, authorize, matters, engineering, produced, relevant, preference, monthly\n",
      "148 150  0.054 (0.364, 0.037) production, victim, allowance, discrimination, reductions, leasing, producer\n",
      "149 101  0.054 (0.358, 0.037) price, items, designed, Division, performing, manufactured, structure, FERC, central\n",
      "150 112  0.053 (0.310, 0.043) defense, Army, strategic, support, military, readiness, capabilities, deployed\n",
      "151 158  0.053 (0.326, 0.040) designated, representative, vote, Cuba, voting, place, volume, timely, highest\n",
      "152  85  0.053 (0.284, 0.046) Imposes, article, hours, authorizing, long-term care, Awareness Week, suppression\n",
      "153 100  0.051 (0.327, 0.039) age, trade, consumers, lives, reducing, NASA, Secretary of Commerce, entering, arms\n",
      "154  83  0.051 (0.348, 0.037) Board, regional, membership, appoint, recommend, Advisory Council, adopted, resident\n",
      "155  29  0.051 (0.315, 0.041) airport, contribution, manufacturers, operations, certificate, air transportation\n",
      "156  70  0.051 (0.373, 0.034) individuals, guidance, Commissioner, Children's, higher education, Syria, household\n",
      "157  21  0.051 (0.304, 0.042) officer, communications, appropriate congressional committees, separation\n",
      "158 186  0.050 (0.326, 0.039) Committee, campaign, obtained, primary, maintaining, establishing, personal\n",
      "159  79  0.050 (0.403, 0.031) entity, assessment, entities, perform, reason, energy efficiency, fewer, 25 percent\n",
      "160 167  0.050 (0.385, 0.033) law, proceedings, adult, civil, outstanding, immigration, later, court, question\n",
      "161  34  0.050 (0.314, 0.040) Iraq, partnership, held, restore, U.S. armed forces, bridge, Maryland, U.N.\n",
      "162 136  0.050 (0.357, 0.035) fiscal year, title, million, contractor, awarded, prescribe, military personnel\n",
      "163 155  0.049 (0.331, 0.037) submission, prohibitions, owned, identifying, determination, agent, grant program\n",
      "164 127  0.049 (0.311, 0.039) adoption, containing, temporary, meeting, legislative, USAID, reported, prescribed\n",
      "165 132  0.049 (0.342, 0.035) permanent, alternative, exemption, disability, described, range, Puerto Rico, select\n",
      "166  80  0.048 (0.338, 0.036) terms, greater, limitation, owner, sell, balance, provisions concerning\n",
      "167 110  0.048 (0.350, 0.034) sale, recovery, registry, local governments, Aging, ownership, displaced, listing\n",
      "168 129  0.047 (0.338, 0.035) commercial, laws, location, FAA, flight, Constitution, conducted, sponsor\n",
      "169  94  0.047 (0.315, 0.037) known, Honors, finding, coal, resource, served, SEA, historic, occurring, Sudan\n",
      "170 109  0.047 (0.294, 0.040) permitting, Colorado, immediate, withholding, denying, coordinating, James\n",
      "171 139  0.046 (0.338, 0.034) safe, treated, damage, reimburse, primarily, supply, gasoline, post, Equity Act\n",
      "172  25  0.046 (0.343, 0.033) work, conference, better, OASDI, departments, shared, pharmacy, Coordinator\n",
      "173  53  0.045 (0.360, 0.031) fees, participating, awards, student, entry, exception, suspension, granted\n",
      "174 138  0.045 (0.310, 0.036) coordination, delivery, domestic violence, July 1, recover, FEMA, passengers\n",
      "175  68  0.044 (0.350, 0.032) centers, reimbursement, expansion, availability, competitiveness, long-term\n",
      "176 115  0.044 (0.353, 0.031) Amendments, extension, employers, reporting requirements, enhanced\n",
      "177 197  0.044 (0.310, 0.035) DOE, placement, furnished, teacher, dedication, obtain, Department of Energy\n",
      "178  41  0.043 (0.313, 0.035) safety, provisions regarding, attack, air, citizen, Amendments Act, public housing\n",
      "179  58  0.043 (0.318, 0.034) testing, biological, utilize, methods, AIDS, partners, maritime, public lands\n",
      "180 102  0.043 (0.314, 0.034) communication, recognize, given, protections, Excellence, electric, social\n",
      "181   4  0.043 (0.316, 0.034) institutions, trust, substance, minority, exclusion, career, industries, scholarship\n",
      "182  92  0.042 (0.337, 0.031) control, sanctions, Iran, approval, deposit, Treaty, acquired\n",
      "183  97  0.042 (0.310, 0.034) identified, rehabilitation, transferred, mental health, need, continuing, relocation\n",
      "184 128  0.042 (0.307, 0.034) damages, Reform Act, appointment, ending, protected, West, delay\n",
      "185 103  0.042 (0.309, 0.034) nation, Foundation, laboratory, database, repayment, achievement, U.S. citizens\n",
      "186 189  0.040 (0.306, 0.032) two years, motion, furnish, second, Homeland Security, meetings, notwithstanding\n",
      "187  81  0.040 (0.300, 0.033) unit, travel, determines, agents, concerning, diseases, preventing, affect\n",
      "188 143  0.039 (0.317, 0.031) engaged, market, Excludes, consent, serving, Florida, unlawful, affecting, writing\n",
      "189  14  0.039 (0.290, 0.033) professional, judicial, orders, hold, expended, rental, Mr., qualify, released\n",
      "190  84  0.038 (0.324, 0.029) two, five years, October 1, guarantee, financing, jointly, enhancement, alternatives\n",
      "191 157  0.038 (0.313, 0.030) protection, carried, longer, higher, lower, adequate, continuation, choice, outcomes\n",
      "192 182  0.036 (0.288, 0.031) received, ten, innovative, public safety, Directs the Attorney General\n",
      "193 161  0.036 (0.335, 0.027) facility, carry, termination, criteria, foreign countries, 60 days, retention, meets\n",
      "194  73  0.036 (0.281, 0.032) January 1, three, threat, remain, capability, gain, Pakistan, commitments, PRC, firm\n",
      "195 165  0.035 (0.331, 0.026) administration, annually, transaction, separate, obligations, applied, obtaining\n",
      "196 126  0.035 (0.322, 0.027) operation, jurisdiction, loss, repair, Terminates, technical assistance, disposition\n",
      "197  30  0.034 (0.291, 0.029) home, operate, demonstration program, declare, collect, ineligible, zone, refund\n",
      "198 152  0.032 (0.274, 0.030) material, taken, municipal, Continues, Russian Federation, senior, radio\n",
      "199  50  0.032 (0.252, 0.031) Subjects, recipient, interim, quantity, retired, tribes, secondary\n"
     ]
    }
   ],
   "source": [
    "print (\"     ID  Score  Prob  Purity  Summary\")\n",
    "for i in range(0,numTopics):\n",
    "    topicID= topicRanking[i]\n",
    "    print (\"%3d %3d %6.3f (%5.3f, %4.3f) %s\" \n",
    "           % (i, topicID, topicScore[topicID], 100*topicProbs[topicID], topicPurity[topicID], topicSummaries[topicID]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    fp = open(\"../Data/CongressionalDocTopicSummaries.tsv\", \"w\")\n",
    "    i = 0\n",
    "    fp.write(\"TopicID\\tTopicSummary\\n\")\n",
    "    for line in topicSummaries:\n",
    "        fp.write(\"%d\\t%s\\n\" % (i, line))\n",
    "        i += 1\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "nbpresent": {
   "slides": {
    "05f2d009-1afe-4edb-8e44-3886ff90f662": {
     "id": "05f2d009-1afe-4edb-8e44-3886ff90f662",
     "prev": "62668528-4383-40d4-8346-55098de21deb",
     "regions": {
      "d017e9eb-469d-4844-a435-8a07fbd34a23": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1931cc98-9c4c-46c4-bc06-ec37c31944f5",
        "part": "whole"
       },
       "id": "d017e9eb-469d-4844-a435-8a07fbd34a23"
      }
     }
    },
    "0cef4fe0-fb8d-4d04-8499-a0aeb8e39c83": {
     "id": "0cef4fe0-fb8d-4d04-8499-a0aeb8e39c83",
     "prev": "b054cb8c-2fae-45e7-a1ea-5ddf5fafd04b",
     "regions": {
      "dd5ebd19-ba29-416a-93fe-a419a7808320": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "55d8b0a5-11a9-4cf1-8399-b2d036254dd0",
        "part": "whole"
       },
       "id": "dd5ebd19-ba29-416a-93fe-a419a7808320"
      }
     }
    },
    "0e6eef49-e6e3-4a88-81ef-9ca6538de3c7": {
     "id": "0e6eef49-e6e3-4a88-81ef-9ca6538de3c7",
     "prev": "91501d8f-6e03-4eda-bc8e-b30ba3dfc5f0",
     "regions": {
      "c8a095f5-9b2e-49ce-932e-d9e6827b0ce0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d3b757c3-d523-4bc0-80af-7be110f7a46e",
        "part": "whole"
       },
       "id": "c8a095f5-9b2e-49ce-932e-d9e6827b0ce0"
      }
     }
    },
    "140f2fe3-3b71-4459-b7b6-6ae13d350899": {
     "id": "140f2fe3-3b71-4459-b7b6-6ae13d350899",
     "prev": "5eea5861-97ec-4b43-9f13-f54553e5f4f8",
     "regions": {
      "add32de8-317f-4069-a350-a458f3c72ccf": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "82e2d906-b067-419d-9532-378c6f36fd69",
        "part": "whole"
       },
       "id": "add32de8-317f-4069-a350-a458f3c72ccf"
      }
     }
    },
    "1776d003-6d9a-4f23-9d96-699132adcd76": {
     "id": "1776d003-6d9a-4f23-9d96-699132adcd76",
     "prev": "53b3e9a4-62db-4c7a-8cfa-fc9f79aee34c",
     "regions": {
      "70125e66-c103-4611-a5da-d3cb44c63b9b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2152a599-0581-4406-a32c-58b4ff36711a",
        "part": "whole"
       },
       "id": "70125e66-c103-4611-a5da-d3cb44c63b9b"
      }
     }
    },
    "1e29821d-1c4f-4da7-bbe5-c979e7357e74": {
     "id": "1e29821d-1c4f-4da7-bbe5-c979e7357e74",
     "prev": "f71b51d6-502c-4fed-9f1c-1ca9c7856bab",
     "regions": {
      "13a49cd8-72d5-4e60-a7fb-17efba769077": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "23d9b7ef-11db-419b-9f97-7bb07a1e3ca5",
        "part": "whole"
       },
       "id": "13a49cd8-72d5-4e60-a7fb-17efba769077"
      }
     }
    },
    "201e343b-d43c-49a4-b269-144375cc209b": {
     "id": "201e343b-d43c-49a4-b269-144375cc209b",
     "prev": "a8f84401-1564-46ed-9f98-3b5c5587fc2b",
     "regions": {
      "2dfbb0e7-5c43-466a-846c-8b9594885821": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bc5e3810-08b3-41aa-af97-0159bf9be597",
        "part": "whole"
       },
       "id": "2dfbb0e7-5c43-466a-846c-8b9594885821"
      }
     }
    },
    "2f493bea-47ab-4331-bb67-b54b8e3dc9d3": {
     "id": "2f493bea-47ab-4331-bb67-b54b8e3dc9d3",
     "prev": "140f2fe3-3b71-4459-b7b6-6ae13d350899",
     "regions": {
      "1a39a72b-ce0e-4369-957c-7e6d4d807e6f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7fd9e75d-7abf-4dca-b173-23405ebdfbf6",
        "part": "whole"
       },
       "id": "1a39a72b-ce0e-4369-957c-7e6d4d807e6f"
      }
     }
    },
    "30262120-abbd-4411-bd89-1b5a439d5e3f": {
     "id": "30262120-abbd-4411-bd89-1b5a439d5e3f",
     "prev": "d41619a3-547d-4c04-be26-a5c33b88d40b",
     "regions": {
      "f4b170ee-2f1b-4b02-8766-a2fc64512189": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "46697539-829d-406f-8722-7b4f7ec3beee",
        "part": "whole"
       },
       "id": "f4b170ee-2f1b-4b02-8766-a2fc64512189"
      }
     }
    },
    "331f88b5-9322-49f1-adc5-75ac27be66dd": {
     "id": "331f88b5-9322-49f1-adc5-75ac27be66dd",
     "prev": "0cef4fe0-fb8d-4d04-8499-a0aeb8e39c83",
     "regions": {
      "d01c1b37-6ee0-4b05-8f98-0fdb1810f1e2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8e615a03-a616-4ce6-aaed-149bb7dc6c41",
        "part": "whole"
       },
       "id": "d01c1b37-6ee0-4b05-8f98-0fdb1810f1e2"
      }
     }
    },
    "34a7e350-fa50-4409-b82e-754894829e6b": {
     "id": "34a7e350-fa50-4409-b82e-754894829e6b",
     "prev": "2f493bea-47ab-4331-bb67-b54b8e3dc9d3",
     "regions": {
      "e155d08e-29ff-43e8-9582-d39d249f8f25": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "688b7bd8-bb00-442d-a195-dd1310424d82",
        "part": "whole"
       },
       "id": "e155d08e-29ff-43e8-9582-d39d249f8f25"
      }
     }
    },
    "363f8c01-8fa5-4d3a-be48-fbe5f510f37f": {
     "id": "363f8c01-8fa5-4d3a-be48-fbe5f510f37f",
     "prev": "bcf6777e-3eda-47d6-b99d-addd1ee0971d",
     "regions": {
      "9bd9bb83-09cf-476f-bf5f-1a626ef786c7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1f6a12bd-9325-457d-83d0-9e374033e64f",
        "part": "whole"
       },
       "id": "9bd9bb83-09cf-476f-bf5f-1a626ef786c7"
      }
     }
    },
    "39047648-1076-4f6f-97a9-18850caefd42": {
     "id": "39047648-1076-4f6f-97a9-18850caefd42",
     "prev": "ceb43268-457f-4903-99d1-30e57078a37e",
     "regions": {
      "505fa6dc-a635-4d40-b72f-dcacbd3abbd6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e537aed2-414b-4e59-be59-5cee147398b5",
        "part": "whole"
       },
       "id": "505fa6dc-a635-4d40-b72f-dcacbd3abbd6"
      }
     }
    },
    "3cda56c3-eb5e-48b8-9212-92a62a2a7826": {
     "id": "3cda56c3-eb5e-48b8-9212-92a62a2a7826",
     "prev": "201e343b-d43c-49a4-b269-144375cc209b",
     "regions": {
      "bd3c7db9-0274-4f8a-8f10-379915d3f716": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3593bde-7eee-4a9d-8565-7d9774947404",
        "part": "whole"
       },
       "id": "bd3c7db9-0274-4f8a-8f10-379915d3f716"
      }
     }
    },
    "46a01693-1013-42d2-8d91-b3712ff3c87a": {
     "id": "46a01693-1013-42d2-8d91-b3712ff3c87a",
     "prev": "05f2d009-1afe-4edb-8e44-3886ff90f662",
     "regions": {
      "175e092d-bf70-405b-863f-015ab6587671": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fd3d5300-60a1-4873-a52d-a9d5b89b3463",
        "part": "whole"
       },
       "id": "175e092d-bf70-405b-863f-015ab6587671"
      }
     }
    },
    "50e992a5-4a41-4ae5-a900-ef487cd4ae35": {
     "id": "50e992a5-4a41-4ae5-a900-ef487cd4ae35",
     "prev": "574d6678-3a5c-4308-9721-08a400ed1c5c",
     "regions": {
      "ed346ef0-2c3c-4f49-8a7f-2d285a89d18f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "61921e49-660e-4d97-a4c8-feec4804e2ce",
        "part": "whole"
       },
       "id": "ed346ef0-2c3c-4f49-8a7f-2d285a89d18f"
      }
     }
    },
    "51573f2c-47b4-4e58-8d26-c5a4bd5bad73": {
     "id": "51573f2c-47b4-4e58-8d26-c5a4bd5bad73",
     "prev": "5eeee22c-1a7f-4886-9d4e-d2dbf6236f7b",
     "regions": {
      "13c66008-6e50-4845-96bc-ff9377c4c983": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4d17de12-2451-4b0b-ab3b-a73bf7f6e9dc",
        "part": "whole"
       },
       "id": "13c66008-6e50-4845-96bc-ff9377c4c983"
      }
     }
    },
    "53b3e9a4-62db-4c7a-8cfa-fc9f79aee34c": {
     "id": "53b3e9a4-62db-4c7a-8cfa-fc9f79aee34c",
     "prev": "363f8c01-8fa5-4d3a-be48-fbe5f510f37f",
     "regions": {
      "c4999ffb-39fa-4078-b348-70fe2e877a91": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0086b4c-7e4a-4bc7-b720-7ef6bf2508ad",
        "part": "whole"
       },
       "id": "c4999ffb-39fa-4078-b348-70fe2e877a91"
      }
     }
    },
    "5475c970-b6a6-472c-8c50-5a792b6d7239": {
     "id": "5475c970-b6a6-472c-8c50-5a792b6d7239",
     "prev": "39047648-1076-4f6f-97a9-18850caefd42",
     "regions": {
      "2118fa40-e217-4d08-81d6-6170d4352e71": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "22d6a20f-fa82-4df0-8aac-29c150397a1f",
        "part": "whole"
       },
       "id": "2118fa40-e217-4d08-81d6-6170d4352e71"
      }
     }
    },
    "54fdedeb-5ba3-4a3b-9434-5848640aec04": {
     "id": "54fdedeb-5ba3-4a3b-9434-5848640aec04",
     "prev": "b6bd6bfd-15ba-42b0-8c9f-0c7da3f2c412",
     "regions": {
      "85dd6a4b-89d5-4d2a-9ec2-b921fb785e59": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe638683-0efb-49b6-93d2-4e8f22f15e6a",
        "part": "whole"
       },
       "id": "85dd6a4b-89d5-4d2a-9ec2-b921fb785e59"
      }
     }
    },
    "565b4230-1376-4e7c-a13e-b4995b8a0bba": {
     "id": "565b4230-1376-4e7c-a13e-b4995b8a0bba",
     "prev": "34a7e350-fa50-4409-b82e-754894829e6b",
     "regions": {
      "ebc439f8-9559-4dee-8a5d-ae06b3d11bf2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ac3b8bc5-6733-46cf-9451-10b7c9ef8d88",
        "part": "whole"
       },
       "id": "ebc439f8-9559-4dee-8a5d-ae06b3d11bf2"
      }
     }
    },
    "574d6678-3a5c-4308-9721-08a400ed1c5c": {
     "id": "574d6678-3a5c-4308-9721-08a400ed1c5c",
     "prev": "46a01693-1013-42d2-8d91-b3712ff3c87a",
     "regions": {
      "bef7e15a-4149-459d-a43e-b10ac158063c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f2b0a490-a67d-47fc-872f-1e4696886ecb",
        "part": "whole"
       },
       "id": "bef7e15a-4149-459d-a43e-b10ac158063c"
      }
     }
    },
    "576b393a-49c9-418c-88c8-d809cf395f9b": {
     "id": "576b393a-49c9-418c-88c8-d809cf395f9b",
     "prev": "f352183b-c020-4e8d-b0c5-22bee23dc2fe",
     "regions": {
      "34336a75-959c-449a-ac26-326f6f712351": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "eaf0b191-c85a-48b9-9bcb-b38ca076c9b4",
        "part": "whole"
       },
       "id": "34336a75-959c-449a-ac26-326f6f712351"
      }
     }
    },
    "57f789db-05dd-4bf0-8526-3c0ad9fe84ef": {
     "id": "57f789db-05dd-4bf0-8526-3c0ad9fe84ef",
     "prev": "a3a6270c-ba38-4a61-9885-cdcefc707993",
     "regions": {
      "10fbe879-f4ba-42f8-a10f-c2d794f23a9a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c158c2fc-56b7-40a7-a765-d7a3b4692839",
        "part": "whole"
       },
       "id": "10fbe879-f4ba-42f8-a10f-c2d794f23a9a"
      }
     }
    },
    "5eea5861-97ec-4b43-9f13-f54553e5f4f8": {
     "id": "5eea5861-97ec-4b43-9f13-f54553e5f4f8",
     "prev": "f7603915-d5b5-4c40-b10a-08bb21bc64bc",
     "regions": {
      "60f7cfd2-412d-45ca-b9f3-288ec1d241d6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b43ca7bd-75e7-4d17-9f77-80ce05375dad",
        "part": "whole"
       },
       "id": "60f7cfd2-412d-45ca-b9f3-288ec1d241d6"
      }
     }
    },
    "5eeee22c-1a7f-4886-9d4e-d2dbf6236f7b": {
     "id": "5eeee22c-1a7f-4886-9d4e-d2dbf6236f7b",
     "prev": "1776d003-6d9a-4f23-9d96-699132adcd76",
     "regions": {
      "b8223200-f30d-44c0-b41e-0a0db1825253": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a726551f-3724-4de9-87d5-0c0418852da4",
        "part": "whole"
       },
       "id": "b8223200-f30d-44c0-b41e-0a0db1825253"
      }
     }
    },
    "62668528-4383-40d4-8346-55098de21deb": {
     "id": "62668528-4383-40d4-8346-55098de21deb",
     "prev": "c1a57d7c-99c7-4e52-812c-cf2c74664e55",
     "regions": {
      "cd307f23-6a10-474d-a0b2-839f698fa5c6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec8066e8-9812-4956-b76a-8a4d0086385c",
        "part": "whole"
       },
       "id": "cd307f23-6a10-474d-a0b2-839f698fa5c6"
      }
     }
    },
    "65d70995-6916-4f25-a4bd-b09969de74ce": {
     "id": "65d70995-6916-4f25-a4bd-b09969de74ce",
     "prev": "3cda56c3-eb5e-48b8-9212-92a62a2a7826",
     "regions": {
      "2637e2e8-d300-488e-8580-f43e05610a39": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "21678e5f-ad6c-46d1-8a71-f64bd9483c42",
        "part": "whole"
       },
       "id": "2637e2e8-d300-488e-8580-f43e05610a39"
      }
     }
    },
    "70e3ee2e-bfe6-4f43-af9c-6346661e54c5": {
     "id": "70e3ee2e-bfe6-4f43-af9c-6346661e54c5",
     "prev": "8aec0b7f-1025-41b7-bc0d-aa8c4448ca70",
     "regions": {
      "f9c404d5-84d6-41b9-baa3-f66373330ff2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bc5ecdb1-c51c-457f-be3f-eee94247f13b",
        "part": "whole"
       },
       "id": "f9c404d5-84d6-41b9-baa3-f66373330ff2"
      }
     }
    },
    "8a4c7762-739d-4b1e-9e82-c1ab40edf5b5": {
     "id": "8a4c7762-739d-4b1e-9e82-c1ab40edf5b5",
     "prev": "565b4230-1376-4e7c-a13e-b4995b8a0bba",
     "regions": {
      "c91c6488-bd1d-4243-bd8b-a5c7b3df186c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e45a357e-ab67-4d85-80e9-d433714f9798",
        "part": "whole"
       },
       "id": "c91c6488-bd1d-4243-bd8b-a5c7b3df186c"
      }
     }
    },
    "8aec0b7f-1025-41b7-bc0d-aa8c4448ca70": {
     "id": "8aec0b7f-1025-41b7-bc0d-aa8c4448ca70",
     "prev": "30262120-abbd-4411-bd89-1b5a439d5e3f",
     "regions": {
      "8b5fc013-d5ba-4eb4-97d7-8332986ca2d9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fb58ed5d-85bb-4ab3-9b8e-2d8b79d2df55",
        "part": "whole"
       },
       "id": "8b5fc013-d5ba-4eb4-97d7-8332986ca2d9"
      }
     }
    },
    "91501d8f-6e03-4eda-bc8e-b30ba3dfc5f0": {
     "id": "91501d8f-6e03-4eda-bc8e-b30ba3dfc5f0",
     "prev": "51573f2c-47b4-4e58-8d26-c5a4bd5bad73",
     "regions": {
      "09a4f864-d729-4241-b38e-f177c8dd729e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c96b2d58-4729-4243-8799-c21dc615fa58",
        "part": "whole"
       },
       "id": "09a4f864-d729-4241-b38e-f177c8dd729e"
      }
     }
    },
    "98743de2-1c1e-48ad-af4c-1f5af339092a": {
     "id": "98743de2-1c1e-48ad-af4c-1f5af339092a",
     "prev": "65d70995-6916-4f25-a4bd-b09969de74ce",
     "regions": {
      "7bf9c7ba-0f21-4335-8e2f-c1167cb2713e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4ddff9a8-0a31-4889-bedb-b64d10eddcc1",
        "part": "whole"
       },
       "id": "7bf9c7ba-0f21-4335-8e2f-c1167cb2713e"
      }
     }
    },
    "9b722714-7a0c-41fe-a2ea-7a1065f9fcfb": {
     "id": "9b722714-7a0c-41fe-a2ea-7a1065f9fcfb",
     "prev": null,
     "regions": {
      "c2409342-142e-4490-99ab-2bd221db4aba": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "31b17f54-6bfd-48da-ade1-d08773cc47f1",
        "part": "whole"
       },
       "id": "c2409342-142e-4490-99ab-2bd221db4aba"
      }
     }
    },
    "a3a6270c-ba38-4a61-9885-cdcefc707993": {
     "id": "a3a6270c-ba38-4a61-9885-cdcefc707993",
     "prev": "a7232942-629c-40bc-a9f6-2843f15aa652",
     "regions": {
      "8c3ed09e-4098-4a54-8d79-f0f3145629b0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f28084b0-cc2e-4231-842f-5765d92d44f0",
        "part": "whole"
       },
       "id": "8c3ed09e-4098-4a54-8d79-f0f3145629b0"
      }
     }
    },
    "a7232942-629c-40bc-a9f6-2843f15aa652": {
     "id": "a7232942-629c-40bc-a9f6-2843f15aa652",
     "prev": "f4146329-5a8f-4afa-b173-bf72e045d2e2",
     "regions": {
      "cbd01583-9fdc-45aa-86fd-66e5acdc068a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6244226e-a91b-4767-83da-d41e882399a4",
        "part": "whole"
       },
       "id": "cbd01583-9fdc-45aa-86fd-66e5acdc068a"
      }
     }
    },
    "a8b9740f-17bf-4874-854a-81138b906f1e": {
     "id": "a8b9740f-17bf-4874-854a-81138b906f1e",
     "prev": "ccb90a10-a80d-478d-b925-2bc0c979183e",
     "regions": {
      "9df434d3-a4fe-4d81-8b5f-2711cf8c285e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bbe5cc20-ad4f-482c-8f94-09de87752d30",
        "part": "whole"
       },
       "id": "9df434d3-a4fe-4d81-8b5f-2711cf8c285e"
      }
     }
    },
    "a8f84401-1564-46ed-9f98-3b5c5587fc2b": {
     "id": "a8f84401-1564-46ed-9f98-3b5c5587fc2b",
     "prev": "70e3ee2e-bfe6-4f43-af9c-6346661e54c5",
     "regions": {
      "0f60d3c1-372d-40d6-b59e-ad867e1c63f6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8a9b81d1-058f-4a37-acbe-b825045866b4",
        "part": "whole"
       },
       "id": "0f60d3c1-372d-40d6-b59e-ad867e1c63f6"
      }
     }
    },
    "b054cb8c-2fae-45e7-a1ea-5ddf5fafd04b": {
     "id": "b054cb8c-2fae-45e7-a1ea-5ddf5fafd04b",
     "prev": "98743de2-1c1e-48ad-af4c-1f5af339092a",
     "regions": {
      "e21ecc9f-a912-45da-96a2-b4d1f970628e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "177ee67d-3e38-4009-b65e-4d08f79c862b",
        "part": "whole"
       },
       "id": "e21ecc9f-a912-45da-96a2-b4d1f970628e"
      }
     }
    },
    "b6bd6bfd-15ba-42b0-8c9f-0c7da3f2c412": {
     "id": "b6bd6bfd-15ba-42b0-8c9f-0c7da3f2c412",
     "prev": "da843dce-021d-471b-a542-f4b8d2591a62",
     "regions": {
      "6550d1d6-e978-43e5-9e79-6f7d4a718cd4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6975bd59-2a7c-466c-b248-7dbff82dea79",
        "part": "whole"
       },
       "id": "6550d1d6-e978-43e5-9e79-6f7d4a718cd4"
      }
     }
    },
    "ba74b38b-6de6-4f16-b7b1-bb78365fee8a": {
     "id": "ba74b38b-6de6-4f16-b7b1-bb78365fee8a",
     "prev": "cf0820e2-9708-4c69-96e1-cfaab18170b6",
     "regions": {
      "a109cf38-42d4-49f1-b5c3-e5d95e13a5f3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5fef2737-5cde-43e2-8c05-04320722da93",
        "part": "whole"
       },
       "id": "a109cf38-42d4-49f1-b5c3-e5d95e13a5f3"
      }
     }
    },
    "bbc708aa-4a8b-4aae-bd36-7dbe87a4172d": {
     "id": "bbc708aa-4a8b-4aae-bd36-7dbe87a4172d",
     "prev": "0e6eef49-e6e3-4a88-81ef-9ca6538de3c7",
     "regions": {
      "306c285e-6439-4773-be85-1d6b90b44dcb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "96008087-cb6e-4ba1-b9c6-b4480835ef0f",
        "part": "whole"
       },
       "id": "306c285e-6439-4773-be85-1d6b90b44dcb"
      }
     }
    },
    "bcf6777e-3eda-47d6-b99d-addd1ee0971d": {
     "id": "bcf6777e-3eda-47d6-b99d-addd1ee0971d",
     "prev": "576b393a-49c9-418c-88c8-d809cf395f9b",
     "regions": {
      "fd7ec647-ff66-4fc9-b026-508f4f73fd73": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bcd1daea-0849-4cd4-9349-a7f52641eb95",
        "part": "whole"
       },
       "id": "fd7ec647-ff66-4fc9-b026-508f4f73fd73"
      }
     }
    },
    "c1a57d7c-99c7-4e52-812c-cf2c74664e55": {
     "id": "c1a57d7c-99c7-4e52-812c-cf2c74664e55",
     "prev": "8a4c7762-739d-4b1e-9e82-c1ab40edf5b5",
     "regions": {
      "1981b742-5951-4860-9e99-bbd9c5ba71a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9c6d2ec8-2fd6-43ac-89e7-293228019bb2",
        "part": "whole"
       },
       "id": "1981b742-5951-4860-9e99-bbd9c5ba71a8"
      }
     }
    },
    "c27308f0-bd6b-4b36-8ff1-6ce863894538": {
     "id": "c27308f0-bd6b-4b36-8ff1-6ce863894538",
     "prev": "cb39cf50-e059-426b-b314-52968b249608",
     "regions": {
      "d9d1c211-e2e4-4242-a9ac-6186c72f7fb9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a9d68eeb-a68b-487f-8fb9-088168cddbda",
        "part": "whole"
       },
       "id": "d9d1c211-e2e4-4242-a9ac-6186c72f7fb9"
      }
     }
    },
    "cb39cf50-e059-426b-b314-52968b249608": {
     "id": "cb39cf50-e059-426b-b314-52968b249608",
     "prev": "a8b9740f-17bf-4874-854a-81138b906f1e",
     "regions": {
      "7d1736b5-d6db-49ea-bb88-d6240a63bf60": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "dacd12f0-444b-4759-af58-27d4ba1c4fe5",
        "part": "whole"
       },
       "id": "7d1736b5-d6db-49ea-bb88-d6240a63bf60"
      }
     }
    },
    "cb89c63b-a2fd-4e72-b76c-7fc44dd7b6be": {
     "id": "cb89c63b-a2fd-4e72-b76c-7fc44dd7b6be",
     "prev": "bbc708aa-4a8b-4aae-bd36-7dbe87a4172d",
     "regions": {
      "d012c252-7d7b-4633-985d-873edc19d5bc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2a72aece-5261-4bad-883b-b21455f957cc",
        "part": "whole"
       },
       "id": "d012c252-7d7b-4633-985d-873edc19d5bc"
      }
     }
    },
    "ccb90a10-a80d-478d-b925-2bc0c979183e": {
     "id": "ccb90a10-a80d-478d-b925-2bc0c979183e",
     "prev": "ba74b38b-6de6-4f16-b7b1-bb78365fee8a",
     "regions": {
      "231f19ab-00e3-47fa-9cd3-cfc74bca54b3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "31b80b11-4918-48ac-a513-974a85830a0a",
        "part": "whole"
       },
       "id": "231f19ab-00e3-47fa-9cd3-cfc74bca54b3"
      }
     }
    },
    "ceb43268-457f-4903-99d1-30e57078a37e": {
     "id": "ceb43268-457f-4903-99d1-30e57078a37e",
     "prev": "331f88b5-9322-49f1-adc5-75ac27be66dd",
     "regions": {
      "f1f8982d-80e7-4081-b787-451ad5a723e7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1301be86-24d2-45a3-aced-d0b4948e765b",
        "part": "whole"
       },
       "id": "f1f8982d-80e7-4081-b787-451ad5a723e7"
      }
     }
    },
    "cf0820e2-9708-4c69-96e1-cfaab18170b6": {
     "id": "cf0820e2-9708-4c69-96e1-cfaab18170b6",
     "prev": "50e992a5-4a41-4ae5-a900-ef487cd4ae35",
     "regions": {
      "b052b2ed-acc4-491e-a9c1-4c418029bacf": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "902f0953-1cc3-465a-a93b-aa08daf2b18b",
        "part": "whole"
       },
       "id": "b052b2ed-acc4-491e-a9c1-4c418029bacf"
      }
     }
    },
    "d41619a3-547d-4c04-be26-a5c33b88d40b": {
     "id": "d41619a3-547d-4c04-be26-a5c33b88d40b",
     "prev": "1e29821d-1c4f-4da7-bbe5-c979e7357e74",
     "regions": {
      "83fa0715-62ef-41ce-a8be-7e7e9f45a4a1": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "44f6c13e-aae6-4bcf-97f5-5527bacc0512",
        "part": "whole"
       },
       "id": "83fa0715-62ef-41ce-a8be-7e7e9f45a4a1"
      }
     }
    },
    "da843dce-021d-471b-a542-f4b8d2591a62": {
     "id": "da843dce-021d-471b-a542-f4b8d2591a62",
     "prev": "cb89c63b-a2fd-4e72-b76c-7fc44dd7b6be",
     "regions": {
      "588d63ad-348c-4393-bff0-7983fbc6fb98": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "98a57c6a-25fc-4597-9098-5521b9153f5b",
        "part": "whole"
       },
       "id": "588d63ad-348c-4393-bff0-7983fbc6fb98"
      }
     }
    },
    "f352183b-c020-4e8d-b0c5-22bee23dc2fe": {
     "id": "f352183b-c020-4e8d-b0c5-22bee23dc2fe",
     "prev": "57f789db-05dd-4bf0-8526-3c0ad9fe84ef",
     "regions": {
      "e60b3552-f330-4638-8dd6-2641f172504f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "be794056-435f-49e2-b69b-2ca5409710bf",
        "part": "whole"
       },
       "id": "e60b3552-f330-4638-8dd6-2641f172504f"
      }
     }
    },
    "f4146329-5a8f-4afa-b173-bf72e045d2e2": {
     "id": "f4146329-5a8f-4afa-b173-bf72e045d2e2",
     "prev": "5475c970-b6a6-472c-8c50-5a792b6d7239",
     "regions": {
      "71d82122-be41-4ecf-b333-56d798905c13": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1940dcd6-9b8b-4dea-af39-ca75e72062aa",
        "part": "whole"
       },
       "id": "71d82122-be41-4ecf-b333-56d798905c13"
      }
     }
    },
    "f71b51d6-502c-4fed-9f1c-1ca9c7856bab": {
     "id": "f71b51d6-502c-4fed-9f1c-1ca9c7856bab",
     "prev": "9b722714-7a0c-41fe-a2ea-7a1065f9fcfb",
     "regions": {
      "1db50964-9f01-4c97-9d5b-778c2bcdb4bf": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "21d8d7fd-501d-4eec-abd4-a827100e5406",
        "part": "whole"
       },
       "id": "1db50964-9f01-4c97-9d5b-778c2bcdb4bf"
      }
     }
    },
    "f7603915-d5b5-4c40-b10a-08bb21bc64bc": {
     "id": "f7603915-d5b5-4c40-b10a-08bb21bc64bc",
     "prev": "54fdedeb-5ba3-4a3b-9434-5848640aec04",
     "regions": {
      "1c520645-43cc-4a54-a8b7-869917f8d407": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "65904084-4721-4b58-8ea5-711b7b6388fa",
        "part": "whole"
       },
       "id": "1c520645-43cc-4a54-a8b7-869917f8d407"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
