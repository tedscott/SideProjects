---
title: "Forest Fire Area Prediction"
output: github_document
---

## Using a few ML algorithms, predict the area of a forest fire based on meteorological conditions, per the data set (<https://archive.ics.uci.edu/ml/datasets/forest+fires>) and approach outlined in [Cortez and Morais, 2007] P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data.

### Compare the efficiency, error, and accuracy of each algorithm employed

### set working directory and load needed packages

```{r, setup}
knitr::opts_knit$set(root.dir = 'C:/Users/tscott/')
```

```{r}
# load packages
library(tidyverse) #for ggplot and dplyr
library(corrplot) #for correlation plots

```

### Get the data

Pre-downloaded from <https://archive.ics.uci.edu/ml/datasets/forest+fires>

```{r}
forestRaw <- read.csv("./OneDrive - Eastside Preparatory School/Courses/DataScience_2/data/forestfires.csv")
```

### From the documentation for the data set

Attribute information:

For more information, read [Cortez and Morais, 2007].

1.  X - x-axis spatial coordinate within the Montesinho park map: 1 to 9
2.  Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9
3.  month - month of the year: "jan" to "dec"
4.  day - day of the week: "mon" to "sun"
5.  FFMC - FFMC index from the FWI system: 18.7 to 96.20
6.  DMC - DMC index from the FWI system: 1.1 to 291.3
7.  DC - DC index from the FWI system: 7.9 to 860.6
8.  ISI - ISI index from the FWI system: 0.0 to 56.10
9.  temp - temperature in Celsius degrees: 2.2 to 33.30
10. RH - relative humidity in %: 15.0 to 100
11. wind - wind speed in km/h: 0.40 to 9.40
12. rain - outside rain in mm/m2 : 0.0 to 6.4
13. area - the burned area of the forest (in ha): 0.00 to 1090.84 (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).

Need to watch out for correlations, e.g., the Initial Spread Index (ISI) is a numeric rating of the expected rate of fire spread. It is based on wind speed and FFMC.

More deets: <https://wikifire.wsl.ch/tiki-indexbdbc.html?page=Canadian+forest+fire+weather+index+system>

### Inspect it and check for correlations

```{r}
summary(forestRaw)

# might want to set the month and day to factors
# but for correlation we can only use numeric values

forestNum <- forestRaw %>% select(-month, -day)
corrs <- cor(forestNum)
corrplot(corrs, order = 'hclust', addrect = 2)
```

### Quite a bit of correlation, especially between temp and the FWI values. Note that there are NO strong correlations with our goal, area

### Having variables that correlate strongly can both make your model worse and make it prone to overfitting - training well on the train data and poor performance on anything else

### When you read the paper by the authors [Cortez and Morais, 2007], you'll notice they took

$ln(area + 1)$

### (area + 1 to avoid log of zero) so the range wasn't so high. Let's do that. But we have to remember to do

$e^x-1$ afterward to get back to actual area

```{r}
forestNum <- forestNum %>% mutate(logArea = log(area+1))
summary(forestNum)
```

## multi-linear regression

### We'll naively try with all the numeric values. We're doing no normalization and not removing any that correlate with others, so we don't expect a great result.

```{r}
# We're going to use 80% of the rows to train the model
# then test it out on the remaining 20% of the data

# Create Training and Test data -
set.seed(732)  # setting seed to reproduce results of random sampling

# create row indices for training data
# sample will choose random row numbers (80% of the total number of rows)
trainingRowIndex <- sample(1:nrow(forestNum), 0.8*nrow(forestNum)) 

# use those row numbers to split up the data into rows for training 
# and rows for testing

# model training data are the rows that sample identified above
trainingData <- forestNum[trainingRowIndex, ]  

# model test data are the remaining rows
# the minus sign says  to take "all rows but those in trainingRowIndex"
testData  <- forestNum[-trainingRowIndex, ]

# make the model
# remember to use the logArea outcome rather than area
mlrModel <- lm(data=trainingData, logArea ~ .-area)
summary(mlrModel)
```

### Wow, only wind among the variables is significant, with high p-values for most 0.05 and an overall p-value of 0.218 with a terrible R-squared as well

### The median residual value is -0.585, and remember that is in log units so not very close to zero. We should look at a Q-Q plot and histogram of the residuals but it is safe to say they won't be good

```{r}
# Q-Q plot is a test for normally distributed residuals (2nd plot in the sequence)
# should be on a 45 deg line with little deviation
plot(mlrModel)

# histogram shows us something similar
ggplot(data=mlrModel, aes(x=mlrModel$residuals)) + geom_histogram()
```

## Finally, let's look at our main measure of goodness we'll be using, the RMSE (root mean squared error)

$RMSE = \sqrt{\frac{\sum(\hat{y}_i - y_i)^2}{n}}$

```{r}
# get values the model predicts for the test set
predictedArea <- predict(mlrModel, newdata=testData)

# calculation root mean squared error
(rmse_mlr <- sqrt(mean((predictedArea - testData$logArea) ^ 2, na.rm = TRUE)))

```

### We'll need that many times, so let's make an RMSE function to use

```{r}
myRMSE <- function(pred, obs) {
  return(sqrt(mean((pred - obs) ^ 2, na.rm = TRUE)))
}


# make sure it works
myRMSE(predictedArea,testData$logArea )
```

### Note that this is the error for the natural logged data and it still really sucks - we're way off of a decent model (which we might have predicted from the high median residuals and terrible stats)

## Do we give up? Heck no! There are other algorithms!

## Decision Tree

```{r}
# packages for decision trees
#install.packages(c("rpart","rpart.plot","rattle"))
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)

# build the tree
start1 <- Sys.time()
myTree <- rpart(data=trainingData, logArea ~ .-area, method='anova')
end1 <- Sys.time()          # Finishing time
(time1 <- end1 - start1) # total time

# check it out and plot the tree
#plot(myTree)
#text(myTree,pretty=0)
summary(myTree)
fancyRpartPlot(myTree)


```

## Check predictions and RMSE

```{r}
# get predictions
predict_tree <- predict(myTree, newdata=testData, type='vector')

# compute RMSE
myRMSE(predict_tree, testData$logArea)

```

### This RMSE is actually worse than the multi-linear model

### Try adjusting the complexity parameter (default model has cp = 0.02246242) from the summary()

```{r}
# check how error and cp vary
# looks like it can't get any better 
# (larger cp would likely produce a single node)
printcp(myTree)
plotcp(myTree)

# prune it with larger and smaller
prunedTreeMoreCP <- prune.rpart(myTree,cp=0.025)
fancyRpartPlot(prunedTreeMoreCP)

prunedTreeLessCP <- prune.rpart(myTree, cp=0.005)
fancyRpartPlot(prunedTreeLessCP)

predicted_tree_largeCP <- predict(prunedTreeMoreCP, testData)
predicted_tree_smallCP <- predict(prunedTreeLessCP, testData)

```

## Check RMSE for each of these pruned trees

```{r}
(rmseBigCP <- myRMSE(predicted_tree_largeCP, testData$logArea))
(rmseSmallCP <- myRMSE(predicted_tree_smallCP, testData$logArea))

```

## Wow, the larger CP tree, that shows only one node, has a slightly better RMSE than the multilinear model (1.377032)!

## Time for Random Forest

```{r}
# libraries
#install.packages("randomForest")
library(randomForest)


# make the forests
start1 <- Sys.time()
myForest <- randomForest(data=trainingData, logArea ~ .-area)
end1 <- Sys.time()          # Finishing time
(time1 <- end1 - start1) # total time

# takes quite a bit longer than decision tree

# check it out 
myForest

# best model?
which.min(myForest$mse)

# RMSE of that model?
sqrt(myForest$mse[which.min(myForest$mse)])

```

### darn, this RMSE is still not as good as multi-linear model. Using 215 trees!

```{r}
# What other amounts can we try based on the error plot?
plot(myForest)

# looks like potentially other candidates but 215 does look to be the min 

# how about variable importance
varImpPlot(myForest)
```

### not surprisingly, temp, RH, and DMC (Duff moisture code, so probably some kind of wetness) are the most important

### Time to adjust some parameters for the random forest

example for tuneRF from <https://www.statology.org/random-forest-in-r/> model_tuned \<- tuneRF( x=airquality[,-1], #define predictor variables y=airquality\$Ozone, #define response variable ntreeTry=500, mtryStart=4, stepFactor=1.5, improve=0.01, trace=FALSE #don't show real-time progress )

```{r}
ncol(trainingData)
str(trainingData)

# we will want to use columns 1-10 for the tuneRF()

# tune it by messing with starting # of trees, identify best mtry and step
tunedForest <- tuneRF(
  x=trainingData[,1:10],
  y=trainingData$logArea,
  ntreeTry=500,
  #mtryStart = 1, #floor(sqrt(ncol(trainingData)-2)),# sqrt number of predictors
  stepFactor = 0.5,
  improve=0.01,
  trace=FALSE
)

# seems like a lower error with mtry=1
myTunedForest <- randomForest(data=trainingData, 
                              logArea ~ .-area, 
                              mtry=1)

# check it out 
myTunedForest

# best model?
which.min(myTunedForest$mse)

# RMSE of that model?
sqrt(myTunedForest$mse[which.min(myTunedForest$mse)])


```

### slightly lower RMSE for the forest with 159 trees instead of 215

### use that tuned model for predictions

```{r}
predictForest <- predict(myTunedForest, testData)

(myTunedForestRMSE <- myRMSE(predictForest, testData$logArea))


```

### TA-DA! A slight improvement in RMSE with this forest!

## Time for XGBoost!

```{r}

## Got some guidance from https://www.statology.org/xgboost-in-r/ 

#install.packages("xgboost")
library(xgboost)

set.seed(732)

# prepare train and test by building matrix objects that XGBoost expects
colnames(trainingData)

# want columns 1-10 as features and 12 as the label (outcome)
train_x <- data.matrix(trainingData[,1:10])
train_y <- trainingData[,12]

test_x <- data.matrix(testData[,1:10])
test_y <- testData[,12]

#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)

#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 50)



```

### best model happened in round 2 with RMSE 1.335, an improvement! Let's build that model and then we can later tweak some parameters.

```{r}
#define final model
final = xgb.train(data = xgb_train, max.depth = 3, nrounds = 2, verbose = 0)

# use it to predict and confirm RMSE
pred_xgb <- predict(final, test_x)

# get RMSE - same as above 1.3355
(myRMSE(pred_xgb, test_y))

# variable importance?
importance_matrix <- xgb.importance(model = final)
xgb.plot.importance(importance_matrix, xlab = "Feature Importance")


```

### we see that XGBoost has penalized several of the features (with its built in L1 and L2 regularization) so that the remaining best features are temperature, wind, DMC, RH, and FFMC

### let's do a grid search to find the best parameters using the code from the r-bloggers post I referenced in class

```{r}

## Code from https://www.r-bloggers.com/2020/11/r-xgboost-regression/

#################
#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(2, 8, 1),
                          eta = seq(.05, .45, .01))
xgb_train_rmse <- NULL
xgb_test_rmse <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(732)
  m_xgb_untuned <- xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 500,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j],
    verbose = FALSE
  )
  
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
  
  #cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(xgb_test_rmse), ]

```

### OK, with those parameters, build that model and allow for more rounds

```{r}

#define final model
finalXGB = xgb.train(data = xgb_train, 
                   max.depth = 2, 
                   watchlist=watchlist, 
                   eta=0.24,
                   lambda = 0.1,
                   alpha = 0.9,
                   nrounds = 500, 
                   verbose = 0)

# round 3 is best testRMSE (1.330)
finalXGB = xgb.train(data = xgb_train, 
                   max.depth = 2, 
                   watchlist=watchlist, 
                   eta=0.24, 
                   nrounds = 4, 
                   verbose = 0)

# use it to predict and confirm RMSE
pred_xgb <- predict(finalXGB, test_x)

# get RMSE - same as above 1.3355
print('The best model RMSE is')
(myRMSE(pred_xgb, test_y))

# variable importance?
importance_matrix <- xgb.importance(model = final)
xgb.plot.importance(importance_matrix, xlab = "Feature Importance")

```

## Hooray, got an even better RMSE with that bit of tuning. Now have a model with a RMSE = 1.321

## Time for a Neural Network!

### Will use the neuralnet package, one of many options in R

Guidance from <https://www.r-bloggers.com/2019/09/r-neural-network/> <https://datascienceplus.com/neuralnet-train-and-test-neural-networks-using-r/> and the vignette for the neuralnet package

```{r}
# install needed package

#install.packages("neuralnet")
library(neuralnet)


```

### Train first network

```{r}
# normalization function
normalizeMe <- function(x) {
  return ((x - min(x))/(max(x) - min(x)))
}

# need to normalize full data set before splitting
forestNumNorm <- as.data.frame(lapply(forestNum, normalizeMe))

# check results
summary(forestNumNorm)
# everything between 0 and 1 as desired

# to save pain, going to drop "area" from data set before splitting, since we're always using logArea
# this means label column is 11, not 12, and features are 1-10
forestNumNorm <- forestNumNorm %>% select(-area)

# split the data as before
set.seed(732)  # same
trainingRowIndex <- sample(1:nrow(forestNumNorm), 0.8*nrow(forestNumNorm)) 
trainDataNN <- forestNumNorm[trainingRowIndex, ]  
testDataNN  <- forestNumNorm[-trainingRowIndex, ]

# build formula for model
n <- colnames(forestNumNorm)[1:10]
f <- as.formula(paste("logArea ~", paste(n[!n %in% "logArea"], collapse = " + ")))

# check formula
(f)

# time this block
start1 <- Sys.time()

# train network with one hidden layer, 5 nodes
m1_nn <- neuralnet(f,
                   data = trainDataNN,
                   hidden = c(5),
                   linear.output = TRUE) # regression

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# predictions
pred_nn <- predict(m1_nn, testDataNN)

# de-normalize predicted output for logArea
yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# ooh, does this work for us? only if I install caret - not now
#postResample(yhat, y)

# visualize it
plot(m1_nn, rep="best")

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))

# not better yet! 1.560 from this model


```

### That model is not better, and also it took \~21 seconds to run. Time to try adding another hidden layer

```{r}

start1 <- Sys.time()

# train network with two hidden layers, 5 nodes, and 2 nodes
m2_nn <- neuralnet(f,
                   data = trainDataNN,
                   hidden = c(5,2),
                   linear.output = TRUE) # regression

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# predictions
pred_nn <- predict(m2_nn, testDataNN)

# de-normalize predicted output for logArea
yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(m2_nn, rep="best")

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))

# Worse! 1.693 from this model

```

## try a basic loop structure to explore hidden layer sizes. Just one layer this time.

```{r}
NN_rmse <- NULL

for(j in seq(2,7,1)) {
  
  start1 <- Sys.time()
  mj_nn <- neuralnet(f,
                   data = trainDataNN,
                   hidden = c(j),
                   linear.output = TRUE) # regression

  pred_nn <- predict(mj_nn, testDataNN)
  yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)
  y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)
  cat(j, "\n")
  NN_rmse[j] <- paste("Network with hidden layer of", j, "nodes. RMSE=", myRMSE(yhat, y))
  
}

# output results
(NN_rmse)

```

### Crazy, the best network with only one layer has 2 nodes. RMSE 1.397 is still not better than multilinear model even. Let's try two hidden layers.

```{r}

# make data frame for results
NN_rmse_df <- data.frame(layer1 = 0, layer2 = 0, RMSE = 0)

#startfull <- Sys.time()

# first layer from 2 - 6 nodes, second from 1 - 3 nodes
for(i in seq(1,6,1)) {
  for(j in seq(0,3,1)) {
    
    #time each model build
    start1 <- Sys.time()
    hid <- ifelse(j==0, c(i), c(i,j)) # to handle case of zero nodes in layer 2
    mj_nn <- neuralnet(f,
                     data = trainDataNN,
                     hidden=hid,
                     linear.output = TRUE) # regression
    time1 <- Sys.time() - start1 
    
    #predictions
    pred_nn <- predict(mj_nn, testDataNN)
    yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)
    y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)
    cat("First layer", i, "second layer", j, "runtime (s)", time1, "\n")
    
    #build up result data
    temp_df <- data.frame(layer1 = i, layer2 = j, RMSE = myRMSE(yhat, y))
    NN_rmse_df <- rbind(NN_rmse_df, temp_df)
    
  }
}

# full runtime
#cat("Entire set took", Sys.time() - startfull, "seconds to run", "\n")

# output results
arrange(NN_rmse_df, RMSE)

```

### Best network model: 1 node in first layer, 3 in second!

```{r}
start1 <- Sys.time()

# train network with two hidden layers, 1 nodes, and 3 nodes
mbest_nn <- neuralnet(f,
                   data = trainDataNN,
                   hidden = c(1,3),
                   linear.output = TRUE) # regression

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# predictions
pred_nn <- predict(mbest_nn, testDataNN)

# de-normalize predicted output for logArea
yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(mbest_nn, rep="best")

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))

```

### we would want to run a cross-validation to get a better error estimate, but it looks like this neural network varies from around 1.365 to 1.41 for RMSE. Not better than XGBoost, and only sometimes better than previous models.

## What if I play with learningrate? And activation function?

```{r}
start1 <- Sys.time()

# train network with two hidden layers, 1 nodes, and 3 nodes
mbest_nn <- neuralnet(f,
                   data = trainDataNN,
                   hidden = c(1,3),
                   linear.output = TRUE,
                   learningrate = 0.01,
                   act.fct = "tanh") # regression

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# predictions
pred_nn <- predict(mbest_nn, testDataNN)

# de-normalize predicted output for logArea
yhat <- pred_nn * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNN[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(mbest_nn, rep="best")

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))

```

### best RMSE from neural network still hovering around 1.37, so we need to try something else to hopefully see a substantial improvement. According to [Cortez and Morais, 2007] P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data, they got the best performance from a Support Vector Machine, by using Support Vector Regression. So, for a final attempt, give that a go.

```{r}
# need e0171 package for svm

#install.packages("e1071")
library(e1071)

# normalized data sets for svm
trainDataNormSVM <- trainDataNN
testDataNormSVM <- testDataNN

# not normalized data
trainDataSVM <- trainingData
testDataSVM <- testData

# build model on norm data, default cost=1 and epsilon=0.1
svmMod <- svm(f, trainDataNormSVM)
print(svmMod)

# predictions to get first rmse
predNormSVM <- predict(svmMod,testDataNormSVM)

# de-normalize predicted output for logArea
yhat <- predNormSVM * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNormSVM[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(svmMod)

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))

```

### not yet better. Time to tune it. Use tune() from e1071 per help from <https://www.svm-tutorial.com/2014/10/support-vector-regression-r/> and <https://www.rdocumentation.org/packages/e1071/versions/1.7-11/topics/tune>

```{r}
start1 <- Sys.time()

# use tune() to tune hyperparameters
tuneResult <- tune(svm, f,  data = trainDataNormSVM, 
                ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9))
  ) 

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# print results
print(tuneResult) # best performance: eps = 0.7, cost = 4
   
# Draw the first tuning graph 
plot(tuneResult)
```

### best parameters epsilon 0.7, cost 4. Re-run with a finer scale near those values.

```{r}
start1 <- Sys.time()

tuneResult <- tune(svm, f,  data = trainDataNormSVM, 
                ranges = list(epsilon = seq(0.6,0.8,0.1), cost = seq(0.1,5,0.1))
  ) 

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# print results
print(tuneResult) # best performance: eps = 0.7, cost = 0.5
   
# Draw the first tuning graph 
plot(tuneResult)
```

### new best result is eps = 0.7, cost = 0.5. Build model with those and check RMSE

```{r}
# build model with those parameters
start1 <- Sys.time()

svmModTuned <- svm(f, trainDataNormSVM, epsilon=0.7, cost=0.5)

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# spit out results
print(svmModTuned)

# predictions to get first rmse
predNormSVM <- predict(svmModTuned,testDataNormSVM)

# de-normalize predicted output for logArea
yhat <- predNormSVM * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNormSVM[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(svmModTuned)

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))


```

### Hmm, 1.346 is a decent RMSE, but not the best as XGBoost got a 1.321. Try the gamma parameter.

```{r}
start1 <- Sys.time()

tuneResult <- tune(svm, f,  data = trainDataNormSVM, 
                ranges = list(epsilon = seq(0.6,0.8,0.1), gamma = 10^(-1:2))
  ) 

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# print results
print(tuneResult) # best performance: eps = 0.7, cost = 0.5
   
# Draw the first tuning graph 
plot(tuneResult)
```

### Hmm, still not much clarity. Trying eps=0.6 and gamma=10 for a model. OK, I did and it wasn't better so manually exploring for a bit...

```{r}
# build model with those parameters
start1 <- Sys.time()

svmModTuned <- svm(f, trainDataNormSVM, epsilon=0.25, cost=0.5, gamma=1e5)

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# spit out results
print(svmModTuned)

# predictions to get first rmse
predNormSVM <- predict(svmModTuned,testDataNormSVM)

# de-normalize predicted output for logArea
yhat <- predNormSVM * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNormSVM[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(svmModTuned)

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))
```

### OK, this seems best. RMSE = 1.3236 is essentially the same as XGBoost. Epsilon=0.25, cost=0.5, gamma=1e5. Time to try with the non-normalized data to see if that makes any difference.

```{r}
# build model on non-normalized data, use parameters from above
svmMod3 <- svm(f, trainDataSVM, epsilon=0.25, cost=0.5, gamma=1e5)
print(svmMod3)

# predictions to get first rmse
predSVM <- predict(svmMod3,testDataSVM)

# de-normalize predicted output for logArea
yhat <- predSVM #* (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataSVM[, 12] #* (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(svmMod3)

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))
```

### SAME! Best I can do, I guess. Winner is XGBoost with 1.321 RMSE on logArea.

## try grid search instead of tune

```{r}
#grid search using denormalized data

hyper_grid <- expand.grid(epsilon = seq(0.3, 0.5, 0.05),
                          cost = seq(0.2, 0.5, 0.05))
min_svm_rmse <- 100
min_epsilon <- 1
min_cost <- 1
min_runtime <- 100

for (j in 1:nrow(hyper_grid)) {
  set.seed(732)
  start1 <- Sys.time()
  svm_m = svm(f, data=trainingData, epsilon = hyper_grid$epsilon[j], cost = hyper_grid$cost[j])
  end1 <- Sys.time() # Finishing time
  runtime <- end1 - start1 # output total time
  
  yhat <- predict(svm_m, testData)
  
  svm_rmse <- myRMSE(testData$logArea, yhat)
  
  if (svm_rmse < min_svm_rmse){
    min_runtime <- runtime
    min_svm_rmse <- svm_rmse
    min_epsilon <- as.numeric(hyper_grid$epsilon[j])
    min_cost <- as.numeric(hyper_grid$cost[j])
  }
}
cat("\n",min_svm_rmse, min_epsilon, min_cost, min_runtime)
```

## OH HO! Finally a better result, and better than XGBoost! And so fast! Much better than tune(). New best RMSE = 1.316 from SVM with epsilon=0.35, cost=0.3

```{r}
# plot results
df <- data.frame(actual = testData$logArea, pred = yhat)
ggplot(df, mapping=aes(x = actual, y = pred)) +
  geom_point() +
  ggtitle('logArea predicted vs actual for best SVM') + geom_smooth(method="lm")
```

## To wrap this up, let's make the best model according to Cortez and Morais. They found a SVM with only the meteorological variables (temp, RH, wind, rain), min-max normalized data, cost = 3, ${\epsilon} = 3{\sigma}\sqrt{\ln{N}/N}$ and ${\gamma} = 2^{-3}$ yielded the best fit, especially for smaller fires, which dominate the data set.

```{r}
# build model with those parameters

# Cortez & Morais model params
eps <- 3*sd(forestNumNorm$logArea)*sqrt(log(nrow(forestNumNorm))/nrow(forestNumNorm))
C <- 3
gam <- 2^-3

start1 <- Sys.time()

svmModCandM <- svm(logArea ~ temp + RH + wind + rain, trainDataNormSVM, epsilon=eps, cost=C, gamma=gam)

end1 <- Sys.time() # Finishing time
(time1 <- end1 - start1) # output total time

# spit out results
print(svmModCandM)

# predictions to get first rmse
predNormSVM <- predict(svmModCandM,testDataNormSVM)

# de-normalize predicted output for logArea
yhat <- predNormSVM * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# same for true values
y <- testDataNormSVM[, 11] * (max(forestNum$logArea) - min(forestNum$logArea)) + min(forestNum$logArea)

# visualize it
plot(svmModCandM)

# get RMSE - best so far is 1.321 from XGBoost
(myRMSE(yhat, y))


```

### well, that model is not better than mine! Guess I should contact them :)